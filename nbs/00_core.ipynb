{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> This module provides a copy of the [Genomes to Fields](https://www.genomes2fields.org/) data for 2014-2021. Data is accesible by the `get_data` function. The other notebooks contain cleaning, imputation, and exploration code. <span style=\"color:red\">_Note:_</span> The provided phenotypic data includes some phenotypes which do not have complete cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is prepared based on internal files and saved into `dataG2F/dataG2F/datasets` from the directories in `nbs_artifacts` where it was saved after generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expose_files = [\n",
    "    'ACGT.npy',\n",
    "    'ACGT_hilb.npy',\n",
    "    'mgmtMatNames.npy',\n",
    "    'mgmtMat.npy',\n",
    "    'SMatNames.npy',\n",
    "    'SMat.npy',\n",
    "    'PlantHarvestNames.npy',\n",
    "    'PlantHarvest.npy',\n",
    "    'WMat.npy',\n",
    "    'WMatNames.npy',\n",
    "    'WMat_hilb.npy',\n",
    "    'phno_geno.csv',\n",
    "    'obs_geno_lookup.npy',\n",
    "    'obs_env_lookup.npy']\n",
    "# input and output names are the same _unless_ the input is a pkl which should be saved as a json\n",
    "expose_files = [[e, e] for e in expose_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pickle as pkl\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACGT.npy\n",
      "ACGT_hilb.npy\n",
      "mgmtMatNames.npy\n",
      "mgmtMat.npy\n",
      "SMatNames.npy\n",
      "SMat.npy\n",
      "PlantHarvestNames.npy\n",
      "PlantHarvest.npy\n",
      "WMat.npy\n",
      "WMatNames.npy\n",
      "WMat_hilb.npy\n",
      "phno_geno.csv\n",
      "obs_geno_lookup.npy\n",
      "obs_env_lookup.npy\n"
     ]
    }
   ],
   "source": [
    "def prep_data(\n",
    "        load_name,\n",
    "        save_name,\n",
    "        load_from = '../nbs_artifacts/05_prep_matrices/',\n",
    "        save_to   = '../dataG2F/datasets/',\n",
    "        force = False\n",
    "        ):\n",
    "    # check if file exists in save location.\n",
    "    if (save_name in os.listdir(save_to)) & (force != True):\n",
    "        pass\n",
    "    else:\n",
    "        # do the load and save name have the same ending? \n",
    "        load_ext = load_name.split('.')[-1]\n",
    "        save_ext = save_name.split('.')[-1]\n",
    "\n",
    "        if load_ext == save_ext:\n",
    "            shutil.copy(\n",
    "                load_from+load_name,\n",
    "                save_to+save_name\n",
    "            )\n",
    "        elif (load_ext == 'pkl') & (save_ext == 'json'):\n",
    "            # assumes pickle is a dictionary\n",
    "            # convert pickle to json and save\n",
    "            with open(load_from+load_name, 'rb') as f:\n",
    "                dat = pkl.load(f)\n",
    "\n",
    "            with open(save_to+save_name, 'w') as f:\n",
    "                json.dump(dat, f, ensure_ascii=False, indent=4)\n",
    "        else:\n",
    "            print(f'Unsure how to move {load_from+load_name} to {save_to+save_name}')\n",
    "\n",
    "\n",
    "for n1, n2 in expose_files:\n",
    "    print(n1)\n",
    "    prep_data(\n",
    "        load_name = n1,\n",
    "        save_name = n2,\n",
    "        load_from = '../nbs_artifacts/05_prep_matrices/',\n",
    "        save_to   = '../dataG2F/datasets/'\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered_kegg_gene_entries.pkl\n",
      "ACGT_gene_slice_list.pkl\n"
     ]
    }
   ],
   "source": [
    "# now rerun for pickled data\n",
    "expose_files = [\n",
    "    'filtered_kegg_gene_entries.pkl',\n",
    "    'ACGT_gene_slice_list.pkl'\n",
    "    ]\n",
    "expose_files = [[e, e] for e in expose_files]\n",
    "\n",
    "\n",
    "for n1, n2 in expose_files:\n",
    "    print(n1)\n",
    "    prep_data(\n",
    "        load_name = n1,\n",
    "        save_name = n2,\n",
    "        load_from = '../nbs_artifacts/07_filter_genotypes/',\n",
    "        save_to   = '../dataG2F/datasets/'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Historical Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "power_data.npy\n",
      "power_date.npy\n",
      "power_lats.npy\n",
      "power_keys.npy\n",
      "power_lons.npy\n"
     ]
    }
   ],
   "source": [
    "expose_files = [\n",
    "    'power_data.npy',\n",
    "    'power_date.npy',\n",
    "    'power_lats.npy',\n",
    "    'power_keys.npy',\n",
    "    'power_lons.npy']\n",
    "# input and output names are the same _unless_ the input is a pkl which should be saved as a json\n",
    "expose_files = [[e, e] for e in expose_files]\n",
    "\n",
    "\n",
    "for n1, n2 in expose_files:\n",
    "    print(n1)\n",
    "    prep_data(\n",
    "        load_name = n1,\n",
    "        save_name = n2,\n",
    "        load_from = '../nbs_artifacts/06_gps_grid_nasa_power/power_data/',\n",
    "        save_to   = '../dataG2F/datasets/'\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping of Historical Data (counties) to gps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move lookup to match county level data to gps coordinates (one to many)\n",
    "prep_data(\n",
    "        load_name = 'latlon_to_county.csv',\n",
    "        save_name = 'latlon_to_county.csv',\n",
    "        load_from = '../nbs_artifacts/06_gps_grid_nasa_power/',\n",
    "        save_to   = '../dataG2F/datasets/'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move a subset of the historical data (y value with the most non-missings)\n",
    "hist = pd.read_csv('../nbs_artifacts/10_collect_historical_data/nass_historical.csv')\n",
    "hist = hist.loc[:, ['State', 'County', 'GRN_BUpACRE', 'Year']]\n",
    "hist.to_csv('../dataG2F/datasets/nass_historical.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data access function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "import pkgutil\n",
    "from io import BytesIO\n",
    "# supported formats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "def get_data(name = '', # `name` of the data to be retrieved. If no recognized name (or '') is passed, a list of available datasets will be printed.\n",
    "             **kwargs # `filename` can be used in lieu of a name\n",
    "             ):\n",
    "    \"This is a simple function to access cleaned and imputed Genomes to Fields data. It's based on my EnvDL.dlfn.g2fc_datawrapper() class but is simpler, not containing methods for setting validation splits, scaling, etc.\"\n",
    "    # if a file name is passed in directly use it.\n",
    "    if 'filename' in kwargs.keys():\n",
    "        filename = kwargs['filename']\n",
    "    else:\n",
    "        # defaults for quick access\n",
    "        defaults_dict = {\n",
    "            ## Genomic Data\n",
    "            'ACGT':         'ACGT.npy',\n",
    "            'ACGT_hilb':    'ACGT_hilb.npy',            \n",
    "            'KEGG_entries': 'filtered_kegg_gene_entries.pkl',\n",
    "            'KEGG_slices':  'ACGT_gene_slice_list.pkl',\n",
    "\n",
    "            ## Soil and Management \n",
    "            'mgmtMatNames': 'mgmtMatNames.npy',\n",
    "            'mgmtMat':      'mgmtMat.npy',\n",
    "            'SMatNames':    'SMatNames.npy',\n",
    "            'SMat':         'SMat.npy',\n",
    "\n",
    "            ## Weather\n",
    "            'PlantHarvestNames': 'PlantHarvestNames.npy',\n",
    "            'PlantHarvest':      'PlantHarvest.npy',\n",
    "            'WMat':              'WMat.npy',\n",
    "            'WMatNames':         'WMatNames.npy',\n",
    "            'WMat_hilb':         'WMat_hilb.npy',\n",
    "\n",
    "            # Response and lookup\n",
    "            'phno':            'phno_geno.csv',\n",
    "            'obs_geno_lookup': 'obs_geno_lookup.npy', # Phno_Idx  Geno_Idx  Is_Phno_Idx\n",
    "            'obs_env_lookup':  'obs_env_lookup.npy',  # Phno_Idx  Env_Idx   Is_Phno_Idx\n",
    "            # 'YMat':            'YMat.npy'\n",
    "\n",
    "            # Historical Weather (NASA Power)\n",
    "            'power_data':'power_data.npy',\n",
    "            'power_date':'power_date.npy',\n",
    "            'power_lats':'power_lats.npy',\n",
    "            'power_keys':'power_keys.npy',\n",
    "            'power_lons':'power_lons.npy',\n",
    "            \n",
    "            # Historical Yield and Metadata\n",
    "            'nass_data':'nass_historical.csv',\n",
    "            'nass_latlon':'latlon_to_county.csv',\n",
    "        }\n",
    "        filename = ''\n",
    "        if name in defaults_dict.keys():\n",
    "            filename = defaults_dict[name]\n",
    "        else: \n",
    "            print(f'`name` not recognized. \\nUse an allowed `name` or specify the filename as a kwarg e.g. `name = \\'\\', filename = \\'demo.txt\\'`\\nAllowed `name`s are:\\n{list(defaults_dict.keys())}')\n",
    "    \n",
    "    if filename == '':\n",
    "        pass\n",
    "    else:\n",
    "        # retrieve the requested data \n",
    "        filetype = filename.split('.')[-1]\n",
    "        x = pkgutil.get_data('dataG2F', f'datasets/{filename}')\n",
    "\n",
    "        if filetype == 'npy':\n",
    "            x = np.load(BytesIO(x))\n",
    "        if filetype == 'csv':\n",
    "            x = pd.read_csv(BytesIO(x))\n",
    "        if filetype == 'pkl':\n",
    "            x = pkl.load(BytesIO(x))\n",
    "        return(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataG2F'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataG2F\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_data\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataG2F'"
     ]
    }
   ],
   "source": [
    "from dataG2F.core import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`name` not recognized. \n",
      "Use an allowed `name` or specify the filename as a kwarg e.g. `name = '', filename = 'demo.txt'`\n",
      "Allowed `name`s are:\n",
      "['ACGT', 'ACGT_hilb', 'KEGG_entries', 'KEGG_slices', 'mgmtMatNames', 'mgmtMat', 'SMatNames', 'SMat', 'PlantHarvestNames', 'PlantHarvest', 'WMat', 'WMatNames', 'WMat_hilb', 'phno', 'obs_geno_lookup', 'obs_env_lookup']\n"
     ]
    }
   ],
   "source": [
    "get_data(name = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[133054,   4873, 123236],\n",
       "       [133055,   4875, 123238],\n",
       "       [133056,   4897, 123684]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_data(name = 'obs_geno_lookup')[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
