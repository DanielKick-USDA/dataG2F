{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aadccc0",
   "metadata": {},
   "source": [
    "# Cluster Genotypes for Training set Definition\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b58a4",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> *NOTE:*</span> This notebook includes visualization of clusters produced with HDBSCAN. At present I'm not using this approach but see [rapid's cuML's documentation](https://docs.rapids.ai/api/cuml/stable/api/#hdbscan) for more information about this method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fd634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "from dataG2F.qol import ensure_dir_path_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4600ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = '../nbs_artifacts/08_cluster_genotypes/'\n",
    "ensure_dir_path_exists(dir_path = cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d714db86",
   "metadata": {},
   "source": [
    "## Load phenotypic data to match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88896122",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from = '../nbs_artifacts/05_prep_matrices/'\n",
    "phno = pd.read_csv(load_from+'phno_geno.csv')\n",
    "phno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42900c0",
   "metadata": {},
   "source": [
    "## Clusters based on HDBSCAN of genomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9308edc-3f9e-4a8d-b3c9-f6c4fe85aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the hdbscan doesn't exist, make it.\n",
    "if not (os.path.exists(cache_path+'hdbscan_acgt_clustering_min_samples.npy') and \n",
    "        os.path.exists(cache_path+'hdbscan_acgt_clustering.npy')):\n",
    "    from cuml.cluster import HDBSCAN # this is in the rapids enviroment\n",
    "    load_from = '../nbs_artifacts/01.03_g2fc_prep_matrices/'\n",
    "    ACGT = np.load(load_from+'ACGT.npy')\n",
    "\n",
    "    n_obs = ACGT.shape[0]\n",
    "    n_len = np.prod(ACGT.shape[1:3])\n",
    "    ACGT = ACGT.swapaxes(1,2  # swap so that when the the array is reshaped it's in the order 1ACGT 2ACGT ... instead of 1A2A3A ...\n",
    "                 ).reshape(n_obs, n_len)    \n",
    "    \n",
    "    min_samples = [(2+i) for i in range(23)]+[25*(1+i) for i in range(10)]\n",
    "\n",
    "    # This may need to be run a few times. Memory on the gpu isn't being freed as soon as it's no longer needed. \n",
    "    # Alternately an arbitrary delay could be used.\n",
    "    for i in tqdm(range(len(min_samples))):\n",
    "        # print(min_samples[i])\n",
    "        save_res_path = cache_path+'hdbscan_acgt_num_'+str(min_samples[i])+'.npy'\n",
    "        if not os.path.exists(save_res_path):\n",
    "            labels_ = HDBSCAN(min_samples=min_samples[i]).fit_predict(ACGT)\n",
    "            np.save(save_res_path, labels_)\n",
    "\n",
    "    temp =  [e for e in os.listdir(cache_path) if re.match('hdbscan_acgt_num_.+', e)]\n",
    "    temp_names = [e.replace('hdbscan_acgt_num_', '').replace('.npy', '') for e in temp]\n",
    "\n",
    "    temp = np.concatenate([np.load(cache_path+e)[:, None] for e in temp], axis = 1)\n",
    "\n",
    "    np.save(cache_path+'hdbscan_acgt_clustering_min_samples.npy', np.array(min_samples))\n",
    "    np.save(cache_path+'hdbscan_acgt_clustering.npy', np.array(temp))\n",
    "\n",
    "else:    \n",
    "    hdbscan_acgt_clustering_min_samples = np.load(cache_path+'hdbscan_acgt_clustering_min_samples.npy')\n",
    "    hdbscan_acgt_clustering = np.load(cache_path+'hdbscan_acgt_clustering.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d9d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(\n",
    "    pd.DataFrame({'num_clusters': list(np.max(hdbscan_acgt_clustering, axis = 0)),\n",
    "                  'samples_in_cluster': list(np.sum(hdbscan_acgt_clustering > -1, axis = 0)),\n",
    "                  'minumum_samples': [e for e in hdbscan_acgt_clustering_min_samples]}),    \n",
    "    x = 'num_clusters', y = 'samples_in_cluster', color = 'minumum_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b7f684",
   "metadata": {},
   "outputs": [],
   "source": [
    "acgt_clustering_df = pd.concat([\n",
    "    # distinct hybrids\n",
    "    phno.loc[:, ['Hybrid', 'Geno_Idx']].drop_duplicates().sort_values('Geno_Idx').reset_index(drop=True),\n",
    "    # clusterings\n",
    "    pd.DataFrame(hdbscan_acgt_clustering, columns=['min_'+str(e) for e in list(hdbscan_acgt_clustering_min_samples)])\n",
    "], axis = 1)\n",
    "\n",
    "acgt_clustering_df[['Female', 'Male']] = acgt_clustering_df['Hybrid'].str.split('/', expand = True)\n",
    "\n",
    "acgt_clustering_df.loc[:, ['Hybrid', 'min_16']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe2e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about in this clustering?\n",
    "temp = acgt_clustering_df.loc[:, ['Female', 'Male', 'min_16']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5347269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many genotypes are in each cluster?\n",
    "len(set(temp.min_16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd1b8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(pd.DataFrame({'cluster': [str(e) for e in list(set(temp.min_16))],\n",
    "              'hybrid_count': [temp.loc[(temp.min_16 == i) , 'min_16'].count() for i in list(set(temp.min_16))]\n",
    "              }).sort_values('hybrid_count', ascending=False),\n",
    "          x = 'cluster', y = 'hybrid_count', markers='marker'\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89e4372",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.loc[temp.min_16 == 107, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d96494",
   "metadata": {},
   "outputs": [],
   "source": [
    "parents_in_cluster = pd.concat([temp.loc[:, ['Female', 'min_16']].rename(columns = {'Female':'Parent'}), \n",
    "           temp.loc[:, ['Male', 'min_16']].rename(columns = {'Male':'Parent'})]\n",
    "         ).assign(n=1).drop_duplicates()\n",
    "\n",
    "px.imshow(\n",
    "parents_in_cluster.pivot(columns='Parent', index='min_16', values='n').reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce995bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How mnay parents are exclusively in one cluster?\n",
    "print('Clusters do not appear to be distinct to prevalent parents (e.g. LH195).\\nIf I use clusters by HDBSCAN, then common parents will show up in many clusters:')\n",
    "parents_in_cluster.groupby(['Parent']).count().sort_values('n', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94e8698",
   "metadata": {},
   "source": [
    "## Clustering based on unique parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807e7cf7-6fbd-4609-97ea-244be91ecdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique Females, Males')\n",
    "[len(set(acgt_clustering_df[e])) for e in ['Female', 'Male']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b0247-a7cf-4c44-b359-ceac182c2a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_parents = list(set(list(set(acgt_clustering_df['Female']))+list(set(acgt_clustering_df['Male']))))\n",
    "print('Unique Parents: '+str(len(uniq_parents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8595a378-94d0-4168-b078-062260ea50df",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = phno.copy()\n",
    "temp[['Female', 'Male']] = temp['Hybrid'].str.split('/', expand = True)\n",
    "temp = temp.loc[:, ['Female', 'Male', 'Yield_Mg_ha']].groupby(['Female', 'Male']).count().reset_index()\n",
    "\n",
    "temp_count = pd.DataFrame({\n",
    "    'uniq_parents': uniq_parents,\n",
    "    'count': [temp.loc[((temp.Female == uniq_parents[i]) | \n",
    "                        (temp.Male == uniq_parents[i])), 'Yield_Mg_ha'].sum() \n",
    "                        for i in range(len(uniq_parents))]})\n",
    "\n",
    "print('The vast majority of samples have one of a few parents')\n",
    "temp_count.sort_values('count', ascending= False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10361644",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(temp_count.sort_values('count', ascending= False), x = 'uniq_parents', y = 'count', log_y=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ce276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(temp.pivot(columns='Male', index='Female', values='Yield_Mg_ha').reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a19d5c",
   "metadata": {},
   "source": [
    "## Train/Test Groups based on Unique parents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23bf0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_count.sort_values('count', ascending= False).head(30).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517224b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = phno.loc[:, ['Year', 'Hybrid']].copy()\n",
    "temp[['Female', 'Male']] = temp['Hybrid'].str.split('/', expand = True)\n",
    "uniq_parents = list(set(list(temp['Female'])+list(temp['Male'])))\n",
    "uniq_years = list(set(list(temp['Year'])))\n",
    "temp = temp.groupby(['Year', 'Female', 'Male']).count().reset_index()\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32bc4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a leaf out of MCMC and set probability to sample a parent to the represenation of that parent in the data set\n",
    "temp_sums = np.array([temp.loc[((temp.Female == e) | (temp.Male == e)), 'Hybrid'].sum() for e in uniq_parents])\n",
    "temp_sums = temp_sums/np.sum(temp_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686432fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tally_cv00(\n",
    "    draw_years = 1,\n",
    "    draw_parents = 1,\n",
    "    seed_val = 234786,\n",
    "    uniq_years = uniq_years,\n",
    "    uniq_parents = uniq_parents, \n",
    "    tally_df = temp, \n",
    "    **kwargs # optional probabilites for np.random.choice for uniq_years, uniq_genos. Should be called p_year, p_parents\n",
    "):\n",
    "    np.random.seed(seed_val)\n",
    "    if 'p_year' in kwargs:\n",
    "        test_years   = list(np.random.choice(uniq_years,   draw_years, replace = False, p = kwargs['p_year']))\n",
    "    else:\n",
    "        test_years   = list(np.random.choice(uniq_years,   draw_years, replace = False))\n",
    "    \n",
    "    if 'p_parents' in kwargs:\n",
    "        test_parents = list(np.random.choice(uniq_parents, draw_parents, replace = False, p = kwargs['p_parents']))\n",
    "    else:\n",
    "        test_parents = list(np.random.choice(uniq_parents, draw_parents, replace = False))\n",
    "\n",
    "    test_year_mask   = tally_df.Year.isin(test_years) \n",
    "    test_Female_mask = tally_df.Female.isin(test_parents)\n",
    "    test_Male_mask   = tally_df.Male.isin(test_parents)\n",
    "\n",
    "    train_mask = ((~(test_year_mask)) & (~(test_Female_mask | test_Male_mask)))\n",
    "    test_mask  = (( (test_year_mask)) & ( (test_Female_mask | test_Male_mask)))\n",
    "\n",
    "    return({\n",
    "        'n_train': tally_df.loc[train_mask, 'Hybrid'].sum(),\n",
    "        'n_test': tally_df.loc[test_mask,  'Hybrid'].sum(), \n",
    "        'test_years': test_years,\n",
    "        'test_parents': test_parents\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f45d9a",
   "metadata": {},
   "source": [
    "### Test large number of individuals first test set sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2acd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling with non-uniform probabilities\n",
    "np.random.seed(347899)\n",
    "\n",
    "n_samples = 1000\n",
    "res_list = []\n",
    "\n",
    "\n",
    "for ith_draw_parents in tqdm([max(1, (5*i)) for i in range(5)]):\n",
    "    for ith_draw_years in [1+i for i in range(2)]:\n",
    "\n",
    "        res = [tally_cv00(\n",
    "            draw_years = ith_draw_years,\n",
    "            draw_parents = ith_draw_parents,\n",
    "            seed_val = e,\n",
    "            uniq_years = uniq_years,\n",
    "            uniq_parents = uniq_parents, \n",
    "            tally_df = temp,\n",
    "        #     p_year\n",
    "            p_parents = temp_sums\n",
    "        ) for e in np.random.randint(1, 10000, n_samples)]\n",
    "\n",
    "        res = pd.DataFrame(zip([e['n_train'] for e in res], [e['n_test'] for e in res]), \n",
    "                     columns= ['n_train', 'n_test'])\n",
    "        res['draw_years'] = ith_draw_years\n",
    "        res['draw_parents'] = ith_draw_parents\n",
    "        res_list += [res]\n",
    "        \n",
    "        \n",
    "res = pd.concat(res_list)\n",
    "# constrain to have the desired test train approximate split\n",
    "# res = res.loc[((0.09 < (res.n_test / (res.n_train + res.n_test))) & \n",
    "#          (0.11 > (res.n_test / (res.n_train + res.n_test)))), ]\n",
    "# px.scatter(res, x = 'draw_parents', y = 'n_test', facet_col='draw_years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e019ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "res['pr_test'] = res.n_test / (res.n_train + res.n_test)\n",
    "px.scatter(res, x = 'n_test', y = 'pr_test', color = 'draw_parents', facet_col='draw_years')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f870c54e",
   "metadata": {},
   "source": [
    "### Test rarity first test set sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acbbe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling with non-uniform probabilities INVERSE to representation (i.e. highest chance of getting non-testers)\n",
    "np.random.seed(347899)\n",
    "\n",
    "n_samples = 1000\n",
    "res_list = []\n",
    "\n",
    "\n",
    "for ith_draw_parents in tqdm([int(np.floor(i)) for i in np.linspace(1, 2203, 15)]):\n",
    "    for ith_draw_years in [1+i for i in range(2)]:\n",
    "\n",
    "        res = [tally_cv00(\n",
    "            draw_years = ith_draw_years,\n",
    "            draw_parents = ith_draw_parents,\n",
    "            seed_val = e,\n",
    "            uniq_years = uniq_years,\n",
    "            uniq_parents = uniq_parents, \n",
    "            tally_df = temp,\n",
    "        #     p_year\n",
    "            p_parents = ((1/temp_sums)/np.sum((1/temp_sums))) # <-- Now inversely weighted by n_obs (sample testers last)\n",
    "        ) for e in np.random.randint(1, 10000, n_samples)]\n",
    "\n",
    "        res = pd.DataFrame(zip([e['n_train'] for e in res], [e['n_test'] for e in res]), \n",
    "                     columns= ['n_train', 'n_test'])\n",
    "        res['draw_years'] = ith_draw_years\n",
    "        res['draw_parents'] = ith_draw_parents\n",
    "        res_list += [res]\n",
    "        \n",
    "        \n",
    "res = pd.concat(res_list)\n",
    "# constrain to have the desired test train approximate split\n",
    "# res = res.loc[((0.09 < (res.n_test / (res.n_train + res.n_test))) & \n",
    "#          (0.11 > (res.n_test / (res.n_train + res.n_test)))), ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ec40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res['pr_test'] = res.n_test / (res.n_train + res.n_test)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece3e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(res, x = 'n_test', y = 'pr_test', color = 'draw_parents', facet_col='draw_years')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca4cc6c",
   "metadata": {},
   "source": [
    "## Demonstrate rarity based test/train/validate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fe03fd",
   "metadata": {},
   "source": [
    "Initial setup: Need a df with the observation counts for environments and genotypes, and unique parents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da03784",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_tally = phno.loc[:, ['Year', 'Hybrid']].copy()\n",
    "parent_tally[['Female', 'Male']] = parent_tally['Hybrid'].str.split('/', expand = True)\n",
    "\n",
    "uniq_parents = list(set(list(parent_tally['Female'])+list(parent_tally['Male'])))\n",
    "uniq_years = list(set(list(parent_tally['Year']))) # this is included for consistency. In practice I will specify the years to be selected from.\n",
    "\n",
    "parent_tally = parent_tally.groupby(['Year', 'Female', 'Male']).count().reset_index()\n",
    "parent_tally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd38fb8",
   "metadata": {},
   "source": [
    "### Generate Train/Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6a93fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weights for each of the allowed parents. Weights should be inversely proportionate to the number of childern each parent has.\n",
    "allowed_parents = uniq_parents \n",
    "\n",
    "allowed_parents_weights = np.array([parent_tally.loc[((parent_tally.Female == e) | (parent_tally.Male == e)), 'Hybrid'].sum() for e in allowed_parents])\n",
    "allowed_parents_weights = allowed_parents_weights/np.sum(allowed_parents_weights) # pr\n",
    "allowed_parents_weights = (1/allowed_parents_weights)/np.sum(1/allowed_parents_weights) # inv -> pr\n",
    "allowed_parents_weights[np.isnan(allowed_parents_weights)] = 0\n",
    "allowed_parents_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1724bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop until an acceptable test/train split is found. Using an large set of seeds instead of a while loop.\n",
    "\n",
    "res = None\n",
    "\n",
    "for i_seed in np.random.randint(1, 10000, 1000):\n",
    "    if res != None:\n",
    "        if ((res['n_test'] > 1000) &  # minimum samples and proportion threshold\n",
    "            (0.09 < (res['n_test'] / (res['n_train'] + res['n_test'])) < 0.11)):\n",
    "#             print('stop')\n",
    "            break\n",
    "\n",
    "    res = tally_cv00(\n",
    "                draw_years = 2,\n",
    "                draw_parents = np.random.randint(\n",
    "                    round(0.1*len(allowed_parents)),\n",
    "                    round(0.5*len(allowed_parents))),\n",
    "                seed_val = i_seed,\n",
    "                uniq_years = [2022, 2021],\n",
    "                uniq_parents = allowed_parents, \n",
    "                tally_df = parent_tally,\n",
    "                p_parents = allowed_parents_weights\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461330fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = res.copy()\n",
    "print(\"Set contains:\\nTest \\tTrain\\n\"+str(res['n_test'])+\"\\t\"+str(res['n_train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d71af6",
   "metadata": {},
   "source": [
    "### Generate validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0798b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update allowed parents to exclude those in the test set and repeat several times for possible validation sets.\n",
    "allowed_parents = [e for e in allowed_parents if e not in res['test_parents']]\n",
    "# disallow years and parents in res\n",
    "mask = ((~(parent_tally.Year.isin(res['test_years']))) & \n",
    "        (~((parent_tally.Female.isin(res['test_parents']) | parent_tally.Male.isin(res['test_parents'])))))\n",
    "# overwrite\n",
    "parent_tally = parent_tally.loc[mask, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613abb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_tally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74418961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any of the allowed parents that are no longer in the parent tally. This will prevent entries with _no_ children from getting undefined weight \n",
    "allowed_parents = [e for e in allowed_parents if e in set(list(parent_tally['Female'])+list(parent_tally['Male']))]\n",
    "\n",
    "# Repeat to generate a validation set\n",
    "allowed_parents_weights = np.array([parent_tally.loc[((parent_tally.Female == e) | (parent_tally.Male == e)), 'Hybrid'].sum() for e in allowed_parents])\n",
    "allowed_parents_weights = allowed_parents_weights/np.sum(allowed_parents_weights) # pr\n",
    "allowed_parents_weights = (1/allowed_parents_weights)/np.sum(1/allowed_parents_weights) # inv -> pr\n",
    "allowed_parents_weights[np.isnan(allowed_parents_weights)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fae91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set_n = 5\n",
    "validation_sets = []\n",
    "\n",
    "for validation_set in range(validation_set_n):\n",
    "    res = None\n",
    "    for i_seed in np.random.randint(1, 10000, 10000):\n",
    "        if res != None:\n",
    "            if ((res['n_test'] > 500) &  # minimum samples and proportion threshold\n",
    "                (0.09 < (res['n_test'] / (res['n_train'] + res['n_test'])) < 0.11)):\n",
    "#                 print('stop')\n",
    "                break\n",
    "\n",
    "        res = tally_cv00(\n",
    "                    draw_years = 2,\n",
    "                    draw_parents = np.random.randint(\n",
    "                        round(0.1*len(allowed_parents)),\n",
    "                        round(0.5*len(allowed_parents))), \n",
    "                    seed_val = i_seed,\n",
    "                    uniq_years = [2014, 2015, 2016, 2017, 2018, 2019, 2020],\n",
    "                    uniq_parents = allowed_parents, \n",
    "                    tally_df = parent_tally,\n",
    "                    p_parents = allowed_parents_weights\n",
    "                )\n",
    "    validation_sets += [res]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad1d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Set contains:\\nTest \\tTrain\\n----    -----\")\n",
    "_ = [print(str(res['n_test'])+\"\\t\"+str(res['n_train'])) for res in validation_sets]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae71400",
   "metadata": {},
   "source": [
    "### Visualize generated results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c58933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate original tally\n",
    "parent_tally = phno.loc[:, ['Year', 'Hybrid']].copy()\n",
    "parent_tally[['Female', 'Male']] = parent_tally['Hybrid'].str.split('/', expand = True)\n",
    "parent_tally = parent_tally.groupby(['Year', 'Female', 'Male']).count().reset_index()\n",
    "parent_tally.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfa06eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['Test']+['v'+str(i) for i in range(len(validation_sets))]\n",
    "\n",
    "for i in range(len(column_names)):\n",
    "    selection_dict = [test_set]+validation_sets\n",
    "    selection_dict = selection_dict[i]\n",
    "    column_name = column_names[i]\n",
    "    \n",
    "    parent_tally[column_name] = 'Excluded'\n",
    "\n",
    "    mask = ((parent_tally.Female.isin(selection_dict['test_parents']) | \n",
    "             parent_tally.Male.isin(selection_dict['test_parents'])) & \n",
    "            (parent_tally.Year.isin(selection_dict['test_years'])))\n",
    "    parent_tally.loc[mask, column_name] = 'Test'\n",
    "\n",
    "    mask = (~(parent_tally.Female.isin(selection_dict['test_parents']) | \n",
    "             parent_tally.Male.isin(selection_dict['test_parents'])) & \n",
    "            ~(parent_tally.Year.isin(selection_dict['test_years'])))\n",
    "    parent_tally.loc[mask, column_name] = 'Train'\n",
    "    \n",
    "    \n",
    "# overwrite any entries excluded by train/test that are flagged as included in a validation set    \n",
    "for i in range(len(column_names)):\n",
    "    column_name = column_names[i]\n",
    "    if column_name != 'Test':\n",
    "        parent_tally.loc[(parent_tally.Test == 'Excluded'), column_name] = 'Excluded'\n",
    "        parent_tally.loc[(parent_tally.Test == 'Test'), column_name] = 'Test'        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a92dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_tally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7bdc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test/train level\n",
    "px.treemap(parent_tally, path=[px.Constant(\"all\"), 'Test', 'Year'], \n",
    "                 values='Hybrid', color='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89883428",
   "metadata": {},
   "outputs": [],
   "source": [
    "treemap_list = [px.treemap(parent_tally, path=[px.Constant(\"all\"), 'Test', e, 'Year'], \n",
    "                 values='Hybrid', color='Test') for e in ['v'+str(i) for i in range(len(validation_sets))]]\n",
    "for e in treemap_list:\n",
    "    e.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502fc866",
   "metadata": {},
   "source": [
    "## Create production Train/test validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97437417",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3489675896)\n",
    "for mk_set in tqdm(range(10)):\n",
    "    \n",
    "    ### Setup ######################################################################################\n",
    "    \n",
    "    parent_tally = phno.loc[:, ['Year', 'Hybrid']].copy()\n",
    "    parent_tally[['Female', 'Male']] = parent_tally['Hybrid'].str.split('/', expand = True)\n",
    "\n",
    "    uniq_parents = list(set(list(parent_tally['Female'])+list(parent_tally['Male'])))\n",
    "    uniq_years = list(set(list(parent_tally['Year']))) # this is included for consistency. In practice I will specify the years to be selected from.\n",
    "\n",
    "    parent_tally = parent_tally.groupby(['Year', 'Female', 'Male']).count().reset_index()\n",
    "\n",
    "\n",
    "    ### Generate Train/Test set ####################################################################\n",
    "\n",
    "    # Calculate weights for each of the allowed parents. Weights should be inversely proportionate to the number of childern each parent has.\n",
    "    allowed_parents = uniq_parents \n",
    "\n",
    "    allowed_parents_weights = np.array([parent_tally.loc[((parent_tally.Female == e) | (parent_tally.Male == e)), 'Hybrid'].sum() for e in allowed_parents])\n",
    "    allowed_parents_weights = allowed_parents_weights/np.sum(allowed_parents_weights) # pr\n",
    "    allowed_parents_weights = (1/allowed_parents_weights)/np.sum(1/allowed_parents_weights) # inv -> pr\n",
    "    allowed_parents_weights[np.isnan(allowed_parents_weights)] = 0\n",
    "\n",
    "\n",
    "    # loop until an acceptable test/train split is found. Using an large set of seeds instead of a while loop.\n",
    "\n",
    "    res = None\n",
    "\n",
    "    for i_seed in np.random.randint(1, 10000, 1000):\n",
    "        if res != None:\n",
    "            if ((res['n_test'] > 1000) &  # minimum samples and proportion threshold\n",
    "                (0.09 < (res['n_test'] / (res['n_train'] + res['n_test'])) < 0.11)):\n",
    "    #             print('stop')\n",
    "                break\n",
    "\n",
    "        res = tally_cv00(\n",
    "                    draw_years = 2,\n",
    "                    draw_parents = np.random.randint(\n",
    "                        round(0.1*len(allowed_parents)),\n",
    "                        round(0.5*len(allowed_parents))),\n",
    "                    seed_val = i_seed,\n",
    "                    uniq_years = [2022, 2021],\n",
    "                    uniq_parents = allowed_parents, \n",
    "                    tally_df = parent_tally,\n",
    "                    p_parents = allowed_parents_weights\n",
    "                )\n",
    "\n",
    "    # convert from numpy ints to base ints for saving as json\n",
    "    res['n_train'] = int(res['n_train'])\n",
    "    res['n_test'] = int(res['n_test'])\n",
    "    res['test_years'] = [int(e) for e in res['test_years']]\n",
    "\n",
    "    test_set = res.copy()\n",
    "    # print(\"Set contains:\\nTest \\tTrain\\n\"+str(res['n_test'])+\"\\t\"+str(res['n_train']))\n",
    "\n",
    "    ### Generate validation sets ###################################################################\n",
    "\n",
    "    # update allowed parents to exclude those in the test set and repeat several times for possible validation sets.\n",
    "    allowed_parents = [e for e in allowed_parents if e not in res['test_parents']]\n",
    "    # disallow years and parents in res\n",
    "    mask = ((~(parent_tally.Year.isin(res['test_years']))) & \n",
    "            (~(parent_tally.Female.isin(res['test_parents']) | parent_tally.Male.isin(res['test_parents']))))\n",
    "    # overwrite\n",
    "    parent_tally = parent_tally.loc[mask, ]\n",
    "\n",
    "\n",
    "    # remove any of the allowed parents that are no longer in the parent tally. This will prevent entries with _no_ children from getting undefined weight \n",
    "    allowed_parents = [e for e in allowed_parents if e in set(list(parent_tally['Female'])+list(parent_tally['Male']))]\n",
    "\n",
    "    # Repeat to generate a validation set\n",
    "    allowed_parents_weights = np.array([parent_tally.loc[((parent_tally.Female == e) | (parent_tally.Male == e)), 'Hybrid'].sum() for e in allowed_parents])\n",
    "    allowed_parents_weights = allowed_parents_weights/np.sum(allowed_parents_weights) # pr\n",
    "    allowed_parents_weights = (1/allowed_parents_weights)/np.sum(1/allowed_parents_weights) # inv -> pr\n",
    "    allowed_parents_weights[np.isnan(allowed_parents_weights)] = 0\n",
    "\n",
    "    validation_set_n = 10\n",
    "    validation_sets = []\n",
    "\n",
    "    for validation_set in range(validation_set_n):\n",
    "        res = None\n",
    "        for i_seed in np.random.randint(1, 10000, 10000):\n",
    "            if res != None:\n",
    "                if ((res['n_test'] > 500) &  # minimum samples and proportion threshold\n",
    "                    (0.09 < (res['n_test'] / (res['n_train'] + res['n_test'])) < 0.11)):\n",
    "    #                 print('stop')\n",
    "                    break\n",
    "\n",
    "            res = tally_cv00(\n",
    "                        draw_years = 2,\n",
    "                        draw_parents = np.random.randint(\n",
    "                            round(0.1*len(allowed_parents)),\n",
    "                            round(0.5*len(allowed_parents))), \n",
    "                        seed_val = i_seed,\n",
    "                        uniq_years = [2014, 2015, 2016, 2017, 2018, 2019, 2020],\n",
    "                        uniq_parents = allowed_parents, \n",
    "                        tally_df = parent_tally,\n",
    "                        p_parents = allowed_parents_weights\n",
    "                    )\n",
    "            # convert from numpy ints to base ints for saving as json\n",
    "        res['n_train'] = int(res['n_train'])\n",
    "        res['n_test'] = int(res['n_test'])\n",
    "        res['test_years'] = [int(e) for e in res['test_years']]\n",
    "        validation_sets += [res]\n",
    "\n",
    "    ### Save out ###################################################################################\n",
    "\n",
    "    # use ctime as a unique id for these\n",
    "    xx = time.localtime()\n",
    "    xx = ':'.join([str(getattr(xx , e)) for e in ['tm_year', 'tm_mon', 'tm_mday', 'tm_hour','tm_min','tm_sec']])\n",
    "\n",
    "    with open(cache_path+xx+'-'+'test'+'.json', 'w') as fp:\n",
    "        json.dump(test_set, fp)\n",
    "\n",
    "    for i in range(len(validation_sets)):\n",
    "        with open(cache_path+xx+'-'+'val'+str(i)+'.json', 'w') as fp:\n",
    "            json.dump(validation_sets[i], fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d9fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all test/val jsons\n",
    "jsons = [e for e in os.listdir(cache_path) if re.match('\\d+:\\d+:\\d+:\\d+:\\d+:\\d+-.+\\.json$', e)]\n",
    "temp_prefixes = [e.replace('-test.json', '') for e in jsons if re.match('.+test\\.json$', e)]\n",
    "temp_prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f83b608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(json_path):\n",
    "    with open(json_path, 'r') as fp:\n",
    "        dat = json.load(fp)\n",
    "    return(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1f6c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = []\n",
    "\n",
    "temp = os.listdir(cache_path)\n",
    "\n",
    "temp_prefix = '2023:9:5:12:8:26'\n",
    "for temp_prefix in temp_prefixes:\n",
    "    print(temp_prefix)\n",
    "    \n",
    "    temp_vals = [e for e in temp if re.match('^'+temp_prefix+'-val\\d+.json$', e)]\n",
    "    temp_vals.sort()\n",
    "\n",
    "\n",
    "    val_n = pd.DataFrame(\n",
    "        [[load_json(json_path= cache_path+temp_val)[e] for e in ['n_train', 'n_test']] for temp_val in temp_vals], \n",
    "        columns=['ValTrain', 'ValTest'])\n",
    "\n",
    "    test_n = pd.DataFrame(\n",
    "        [[load_json(json_path= cache_path+temp_prefix+'-test.json')[e] for e in ['n_train', 'n_test']]], \n",
    "        columns=['Train', 'Test'])\n",
    "\n",
    "    val_n['Train'] = int(test_n['Train']) \n",
    "    val_n['Test'] = int(test_n['Test']) \n",
    "    val_n['Prefix'] = temp_prefix\n",
    "    \n",
    "    temp_list += [val_n]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea6995",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp =  pd.concat(temp_list).reset_index(drop = True)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9d68cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.melt(id_vars=['Prefix'], value_vars=['ValTrain', 'ValTest', 'Train', 'Test']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fca54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(temp, x = 'Prefix', y = 'value', color = 'variable' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
