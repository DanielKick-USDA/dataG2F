{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7219b3",
   "metadata": {},
   "source": [
    "# Deep Learning Convenience Functions\n",
    "\n",
    "> This notebook contains convenience functions to aid in modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32788a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dlfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7060d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, re\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from EnvDL.core import read_json\n",
    "from EnvDL.dna import np_3d_to_hilbert\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "from graphviz import Digraph # used in VNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b58d051",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8510e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def calc_cs(x # numeric array\n",
    "           ): \n",
    "    \"Calculate nan mean and nan std of an array. Returned as list\"\n",
    "    return [np.nanmean(x, axis = 0), np.nanstd(x, axis = 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b0a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def apply_cs(xs, \n",
    "             cs_dict_entry # list of length 2 containing mean and s\n",
    "            ): return ((xs - cs_dict_entry[0]) / cs_dict_entry[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9321aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def reverse_cs(xs, cs_dict_entry): return (cs_dict_entry[1] * xs) + cs_dict_entry[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c133332b",
   "metadata": {},
   "source": [
    "## Train/Validate/Test Split info\n",
    "\n",
    "Stored as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe75151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def read_split_info(\n",
    "    load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/',\n",
    "    json_prefix = '2023:9:5:12:8:26'):\n",
    "    \"\"\n",
    "    jsons = [e for e in os.listdir(load_from) if re.match('^'+json_prefix+'.+\\.json$', e)]\n",
    "    vals = [e for e in jsons if re.match('.+val\\d+\\.json$', e)]\n",
    "    vals.sort()\n",
    "    out = {}\n",
    "    out['test'] = [read_json(json_path = load_from+json_prefix+'-test.json')]\n",
    "    out['test_file'] = [json_prefix+'-test.json']\n",
    "    out['validate'] = [read_json(json_path = load_from+val) for val in vals]\n",
    "    out['validate_files'] = [val for val in vals]\n",
    "    return(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051b1a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def find_idxs_split_dict(\n",
    "    obs_df, # assumes presence of Year, Female, Male\n",
    "    split_dict # from read_split_info() output. Should be a test of validate dict.\n",
    "):\n",
    "    temp = obs_df\n",
    "    test_mask = ((temp.Year.isin(split_dict['test_years'])) & \n",
    "                 ((temp.Female.isin(split_dict['test_parents'])) |\n",
    "                  (temp.Male.isin(split_dict['test_parents']))))\n",
    "    temp['Split'] = ''\n",
    "    temp.loc[test_mask, 'Split'] = 'Test'\n",
    "\n",
    "    train_mask = (~(temp.Year.isin(split_dict['test_years'])) & \n",
    "                 (~((temp.Female.isin(split_dict['test_parents'])) |\n",
    "                  (temp.Male.isin(split_dict['test_parents'])))))\n",
    "    temp.loc[train_mask, 'Split'] = 'Train'\n",
    "\n",
    "    temp_test  = (temp.Split == 'Test') # should be the same as with the mask above\n",
    "    temp_train = (temp.Split == 'Train') # should be the same as with the mask above\n",
    "\n",
    "    # Confirm that there's no overlap in parents or years\n",
    "    temp_test_parents  = set(temp.loc[temp_test, 'Female']+temp.loc[temp_test, 'Male'])\n",
    "    temp_train_parents = set(temp.loc[temp_train, 'Female']+temp.loc[temp_train, 'Male'])\n",
    "\n",
    "    temp_test_years  = set(temp.loc[temp_test, 'Year'])\n",
    "    temp_train_years = set(temp.loc[temp_train, 'Year'])\n",
    "\n",
    "    assert [] == [e for e in temp_test_parents if e in temp_train_parents]\n",
    "    assert [] == [e for e in temp_train_parents if e in temp_test_parents]\n",
    "    assert [] == [e for e in temp_test_years if e in temp_train_years]\n",
    "    assert [] == [e for e in temp_train_years if e in temp_test_years]\n",
    "\n",
    "    return({\n",
    "        'test_idx': temp.loc[test_mask, ].index, \n",
    "        'train_idx': temp.loc[train_mask, ].index} )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be51f17",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c2b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def LSUV_(model, data, apply_only_to=['Conv', 'Linear', 'Bilinear'],\n",
    "          std_tol=0.1, max_iters=10, do_ortho_init=True, logging_FN=print):\n",
    "    r\"\"\"\n",
    "    Refer to https://github.com/glassroom/torch_lsuv_init\n",
    "    Applies layer sequential unit variance (LSUV), as described in\n",
    "    `All you need is a good init` - Mishkin, D. et al (2015):\n",
    "    https://arxiv.org/abs/1511.06422\n",
    "\n",
    "    Args:\n",
    "        model: `torch.nn.Module` object on which to apply LSUV.\n",
    "        data: sample input data drawn from training dataset.\n",
    "        apply_only_to: list of strings indicating target children\n",
    "            modules. For example, ['Conv'] results in LSUV applied\n",
    "            to children of type containing the substring 'Conv'.\n",
    "        std_tol: positive number < 1.0, below which differences between\n",
    "            actual and unit standard deviation are acceptable.\n",
    "        max_iters: number of times to try scaling standard deviation\n",
    "            of each children module's output activations.\n",
    "        do_ortho_init: boolean indicating whether to apply orthogonal\n",
    "            init to parameters of dim >= 2 (zero init if dim < 2).\n",
    "        logging_FN: function for outputting progress information.\n",
    "\n",
    "    Example:\n",
    "        >>> model = nn.Sequential(nn.Linear(8, 2), nn.Softmax(dim=1))                                                                                                                                                                                                                                            \n",
    "        >>> data = torch.randn(100, 8)\n",
    "        >>> LSUV_(model, data)\n",
    "    \"\"\"\n",
    "\n",
    "    matched_modules = [m for m in model.modules() if any(substr in str(type(m)) for substr in apply_only_to)]\n",
    "\n",
    "    if do_ortho_init:\n",
    "        logging_FN(f\"Applying orthogonal init (zero init if dim < 2) to params in {len(matched_modules)} module(s).\")\n",
    "        for m in matched_modules:\n",
    "            for p in m.parameters():                \n",
    "                torch.nn.init.orthogonal_(p) if (p.dim() >= 2) else torch.nn.init.zeros_(p)\n",
    "\n",
    "    logging_FN(f\"Applying LSUV to {len(matched_modules)} module(s) (up to {max_iters} iters per module):\")\n",
    "\n",
    "    def _compute_and_store_LSUV_stats(m, inp, out):\n",
    "        m._LSUV_stats = { 'mean': out.detach().mean(), 'std': out.detach().std() }\n",
    "\n",
    "    was_training = model.training\n",
    "    model.train()  # sets all modules to training behavior\n",
    "    with torch.no_grad():\n",
    "        for i, m in enumerate(matched_modules):\n",
    "            with m.register_forward_hook(_compute_and_store_LSUV_stats):\n",
    "                for t in range(max_iters):\n",
    "                    _ = model(data)  # run data through model to get stats\n",
    "                    mean, std = m._LSUV_stats['mean'], m._LSUV_stats['std']\n",
    "                    if abs(std - 1.0) < std_tol:\n",
    "                        break\n",
    "                    m.weight.data /= (std + 1e-6)\n",
    "            logging_FN(f\"Module {i:2} after {(t+1):2} itr(s) | Mean:{mean:7.3f} | Std:{std:6.3f} | {type(m)}\")\n",
    "            delattr(m, '_LSUV_stats')\n",
    "\n",
    "    if not was_training: model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4957d8c",
   "metadata": {},
   "source": [
    "## Network Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fe1654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def Linear_block(in_size, out_size, drop_pr):\n",
    "    block = nn.Sequential(\n",
    "        nn.Linear(in_size, out_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(drop_pr)\n",
    "    )\n",
    "    return(block) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6214c2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbe6221",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def Conv1D_x2_Max_block(in_channels, out_channels, kernel_size, stride, maxpool_size):\n",
    "    block = nn.Sequential(\n",
    "        nn.Conv1d(\n",
    "            in_channels= in_channels, # second channel\n",
    "            out_channels= out_channels,\n",
    "            kernel_size= kernel_size,\n",
    "            stride= stride\n",
    "        ), \n",
    "        nn.Conv1d(\n",
    "            in_channels= out_channels, \n",
    "            out_channels= out_channels,\n",
    "            kernel_size= kernel_size,\n",
    "            stride= stride\n",
    "        ), \n",
    "        nn.BatchNorm1d(out_channels),\n",
    "        nn.MaxPool1d((maxpool_size), stride=stride)\n",
    "    )\n",
    "    return(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bbdff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25d2f79c",
   "metadata": {},
   "source": [
    "### Resnet Blocks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf58d524",
   "metadata": {},
   "source": [
    "#### Resnet For 4d (b, c, h, w) (2d conv)\n",
    "4d is shown first because this code was adapted from the pytorch implementation of ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c96b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af338c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1):\n",
    "    # Using https://pytorch.org/vision/main/_modules/torchvision/models/resnet.html as a starting point\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb3ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1):\n",
    "\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c2282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class BasicBlock2d(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: nn.Module = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: nn.Module = None,\n",
    "        expansion: int = 1\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # Set up defaults if none was passed in\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width !=  64: raise ValueError('Only groups = 1 and base_width = 64 supported')\n",
    "        if dilation > 1: raise NotImplementedError('Dilation not supported')\n",
    "\n",
    "        # self.expansion = expansion # in ResNet v1, this is 1, in ResNet v1.5 it is set to 4\n",
    "\n",
    "        # self.conv1 = conv3x3(inplanes, planes*self.expansion, stride)\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # self.conv2 = conv3x3(planes, planes*self.expansion)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b8090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class BottleneckBlock2d(nn.Module):\n",
    "    # This is the block used in ResNet v1.5. It is supposed to be more effective.\n",
    "    # Main changes are that \n",
    "    # - expansion is not set to 1 \n",
    "    # - now there is a third convolution that happens in the slow path\n",
    "    #\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: nn.Module = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: nn.Module = None,\n",
    "        expansion: int = 4\n",
    "    ) -> None:        \n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "            \n",
    "        self.expansion = expansion\n",
    "\n",
    "        width = int(planes * (base_width / 64.0)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7801a723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class ResNet2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block, #: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers, #: List[int],\n",
    "        num_outputs: int = 1,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation = None, #: Optional[List[bool]] = None,\n",
    "        norm_layer = None, #: Optional[Callable[..., nn.Module]] = None,\n",
    "        input_channels = 4\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # _log_api_usage_once(self)\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\n",
    "                \"replace_stride_with_dilation should be None \"\n",
    "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
    "            )\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(input_channels, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False) # Note that this is 4 not 3\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_outputs)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck) and m.bn3.weight is not None:\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "                elif isinstance(m, BasicBlock) and m.bn2.weight is not None:\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block, #: Type[Union[BasicBlock, Bottleneck]],\n",
    "        planes: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1,\n",
    "        dilate: bool = False,\n",
    "    ) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a16055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d404cbb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "568521e6",
   "metadata": {},
   "source": [
    "#### Resnet For 3d (b, c, l) (1d conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72279530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def conv0x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv1d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dfbbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def conv0x1(in_planes: int, out_planes: int, stride: int = 1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f921612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class BasicBlock1d(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: nn.Module = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: nn.Module = None,\n",
    "        expansion: int = 1\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # Set up defaults if none was passed in\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm1d\n",
    "        if groups != 1 or base_width !=  64: raise ValueError('Only groups = 1 and base_width = 64 supported')\n",
    "        if dilation > 1: raise NotImplementedError('Dilation not supported')\n",
    "\n",
    "        # self.expansion = expansion # in ResNet v1, this is 1, in ResNet v1.5 it is set to 4\n",
    "\n",
    "        # self.conv1 = conv3x3(inplanes, planes*self.expansion, stride)\n",
    "        self.conv1 = conv0x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # self.conv2 = conv3x3(planes, planes*self.expansion)\n",
    "        self.conv2 = conv0x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9195dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "  \n",
    "class BottleneckBlock1d(nn.Module):\n",
    "    # This is the block used in ResNet v1.5. It is supposed to be more effective.\n",
    "    # Main changes are that \n",
    "    # - expansion is not set to 1 \n",
    "    # - now there is a third convolution that happens in the slow path\n",
    "    #\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: nn.Module = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: nn.Module = None,\n",
    "        expansion: int = 4\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm1d\n",
    "            \n",
    "        self.expansion = expansion\n",
    "\n",
    "        width = int(planes * (base_width / 64.0)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv0x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv0x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv0x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ba1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ResNet1d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block, #: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers, #: List[int],\n",
    "        num_outputs: int = 1,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation = None, #: Optional[List[bool]] = None,\n",
    "        norm_layer = None, #: Optional[Callable[..., nn.Module]] = None,\n",
    "        input_channels = 4\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # _log_api_usage_once(self)\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm1d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\n",
    "                \"replace_stride_with_dilation should be None \"\n",
    "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
    "            )\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv1d(input_channels, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False) # Note that this is 4 not 3\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d((1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_outputs)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm1d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck) and m.bn3.weight is not None:\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "                elif isinstance(m, BasicBlock) and m.bn2.weight is not None:\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block, #: Type[Union[BasicBlock, Bottleneck]],\n",
    "        planes: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1,\n",
    "        dilate: bool = False,\n",
    "    ) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv0x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e300a71",
   "metadata": {},
   "source": [
    "## Visible Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f239d",
   "metadata": {},
   "source": [
    "### Example usage of VNNHelper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f343bb5",
   "metadata": {},
   "source": [
    "#### Use with a single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b81262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VNNHelper():\n",
    "    def __init__(self, edge_dict, all_values_are_nodes = True) -> None:\n",
    "        self.edge_dict = edge_dict.copy()\n",
    "        # handles updating all the lists of node groups (imp, out, edge)\n",
    "        self._setup_wrapper(all_values_are_nodes)\n",
    "\n",
    "        self.node_props = {}\n",
    "        for e in self.node_keys_vals['all_keys']:\n",
    "            self.node_props[e] = {}\n",
    "\n",
    "    def _setup_wrapper(self, all_values_are_nodes):\n",
    "        if all_values_are_nodes:\n",
    "            self._init_missing_nodes_as_input()\n",
    "        self.node_keys_vals = self._find_uniq_keys_values(input_dict = self.edge_dict)\n",
    "        self.nodes_inp = self._find_nodes_inp(all_key_value_dict = self.node_keys_vals)\n",
    "        self.nodes_out = self._find_top_nodes(all_key_value_dict = self.node_keys_vals)\n",
    "        self.nodes_edge= self._find_nodes_edge(all_key_value_dict= self.node_keys_vals, nodes_inp= self.nodes_inp, nodes_out = self.nodes_out)\n",
    "        # sets self.dependancy_order\n",
    "        self._get_run_order()\n",
    "\n",
    "    # CRUD operations on node proprty dictionary\n",
    "    def node_prop_create(self, name):\n",
    "        self.node_props[name] = {}\n",
    "        \n",
    "    def node_prop_read_keys(self, name):\n",
    "        return self.node_props[name].keys()\n",
    "\n",
    "    def node_prop_read_values(self, name, key):\n",
    "        return self.node_props[name][key]\n",
    "\n",
    "    def node_prop_update(self, name, key, value):\n",
    "        self.node_props[name][key] = value\n",
    "\n",
    "    def node_prop_delete(self, name, key=None):\n",
    "        if key == None:\n",
    "            del self.node_props[name]\n",
    "        else:\n",
    "            del self.node_props[name][key]\n",
    "\n",
    "\n",
    "    def _find_uniq_keys_values(self, input_dict):\n",
    "        \"\"\"\n",
    "        Building a Neural Net from an arbitrary graph\n",
    "        start by finding the top level -- all those keys which are theselves not values\n",
    "        helper function to get all keys and all value from a dict. Useful for when keys don't have unique values.\n",
    "        \"\"\"\n",
    "        all_keys = list(input_dict.keys())\n",
    "        all_values = []\n",
    "        for e in all_keys:\n",
    "            all_values.extend(input_dict[e])\n",
    "        all_values = list(set(all_values))\n",
    "\n",
    "        return({'all_keys': all_keys, 'all_values': all_values})\n",
    "\n",
    "    def _find_top_nodes(self, all_key_value_dict):\n",
    "        \"\"\"\n",
    "        Find order that nodes in the graph should be called to have all dependencies run when they are called.\n",
    "        find the dependancies for run order from many dependancies to none\n",
    "        wrapper function to find the nodes that aren't any other nodes dependancies.\n",
    "        \"\"\"\n",
    "        return([e for e in all_key_value_dict['all_keys'] if e not in all_key_value_dict['all_values']])\n",
    "    \n",
    "    def _init_missing_nodes_as_input(self):\n",
    "        node_keys_vals = self._find_uniq_keys_values(input_dict = self.edge_dict)\n",
    "        add_these_nodes = [e for e in node_keys_vals['all_values'] if e not in node_keys_vals['all_keys']]\n",
    "        for e in add_these_nodes:\n",
    "            self.edge_dict[e] = []\n",
    "\n",
    "    def _find_nodes_inp(self, all_key_value_dict):\n",
    "        # \"\"\"\n",
    "        # wrapper function to find the input nodes. They don't occur in the keys and thus won't be added to the list otherwise.\n",
    "        # another way to do this would have been to \n",
    "        # \"\"\"\n",
    "        # return([e for e in all_key_value_dict['all_values'] if e not in all_key_value_dict['all_keys']])\n",
    "        return [e for e in all_key_value_dict['all_keys'] if self.edge_dict[e] == []]\n",
    "\n",
    "    def _find_nodes_edge(self, all_key_value_dict, nodes_inp, nodes_out):\n",
    "        return [e for e in all_key_value_dict['all_keys'] if e not in nodes_inp+nodes_out]\n",
    "\n",
    "    def append_output_node(self, node_name):\n",
    "        if node_name in self.edge_dict.keys():\n",
    "            pass\n",
    "        else:\n",
    "            self.edge_dict[node_name] = self.nodes_out\n",
    "            self._setup_wrapper()\n",
    "            self.node_prop_create(name = node_name)\n",
    "\n",
    "    def _get_run_order(self, max_iter = 1000):\n",
    "        temp = self.edge_dict.copy()\n",
    "        dependancy_order = []\n",
    "        # Then iterate\n",
    "        for _ in range(max_iter): \n",
    "            top_nodes = self._find_top_nodes(all_key_value_dict = self._find_uniq_keys_values(input_dict = temp))\n",
    "            if top_nodes == []:\n",
    "                break\n",
    "            else:\n",
    "                dependancy_order += top_nodes    \n",
    "                # remove nodes from the graph that are at the 'top' level and haven't already been removed\n",
    "                for key in [e for e in dependancy_order if e in temp.keys()]:\n",
    "                    temp.pop(key)\n",
    "\n",
    "        # reverse to get the order that the nodes should be called\n",
    "        dependancy_order.reverse()                \n",
    "        self.dependancy_order = dependancy_order\n",
    "\n",
    "\n",
    "    def set_node_props(self, key, node_val_zip):\n",
    "        for name, val in node_val_zip:\n",
    "            self.node_prop_update(name = name, key=key, value=val)\n",
    "    \n",
    "    def calc_edge_inp(self):\n",
    "        for name in self.nodes_edge + self.nodes_out: \n",
    "            inp_size = sum([self.node_prop_read_values(name = e, key= 'out') for e in self.edge_dict[name]])\n",
    "            self.node_prop_update(name = name, key='inp', value=inp_size)     \n",
    "\n",
    "    def mk_digraph(self, include = ['node_name', 'inp_size', 'out_size']):\n",
    "        dot = ''\n",
    "        dot = Digraph()\n",
    "        for key in self.node_props.keys():\n",
    "            key_label = []\n",
    "\n",
    "            if 'node_name' in include: key_label += [key]\n",
    "            if 'inp_size' in include: key_label += ['In  '+str(self.node_props[key]['inp'])]\n",
    "            if 'out_size' in include: key_label += ['Out '+str(self.node_props[key]['out'])]\n",
    "            \n",
    "            if len(key_label) == 0:\n",
    "                key_label = ''\n",
    "            else:\n",
    "                key_label = '\\n'.join(key_label)\n",
    "\n",
    "            dot.node(key, key_label)\n",
    "            for value in self.edge_dict[key]:\n",
    "                # edge takes a head/tail whereas edges takes name pairs concatednated (A, B -> AB)in a list\n",
    "                dot.edge(value, key)    \n",
    "        return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d903df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing on example version \n",
    "\n",
    "n_obs = 100 # 100 obs for each group\n",
    "y_true = torch.from_numpy(np.concatenate([\n",
    "        np.zeros((n_obs, )),\n",
    "        np.ones( (n_obs, ))], 0)) + .1* torch.rand(2*n_obs,)\n",
    "\n",
    "input_tensor_dict = {\n",
    "    'in1': torch.from_numpy(np.concatenate([\n",
    "        np.zeros((n_obs, 4, 3)),\n",
    "        np.ones( (n_obs, 4, 3))], 0)),\n",
    "    'in2': torch.from_numpy(np.concatenate([\n",
    "        np.zeros((n_obs, 4, 2)),  \n",
    "        np.ones( (n_obs, 4, 2))], 0))}\n",
    "\n",
    "x_list_temp = [input_tensor_dict[key].to(torch.float) for key in input_tensor_dict.keys()]\n",
    "[e.shape for e in x_list_temp]\n",
    "\n",
    "\n",
    "kegg_connections = {\n",
    "    'y_hat':['d', 'c'],\n",
    "    'd':['b'],\n",
    "    'c':['a'],\n",
    "    'b':['in2', 'a'],\n",
    "    'a':['in1'],\n",
    "    'in1': [],\n",
    "    'in2': []\n",
    "}\n",
    "\n",
    "myvnn = VNNHelper(edge_dict = kegg_connections)\n",
    "\n",
    "myvnn.nodes_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3efbd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init input node sizes\n",
    "inp_tensor_name_to_size = zip(\n",
    "    ['in1', 'in2'], \n",
    "    [int(torch.prod(torch.tensor(e.shape)[1:])) for e in x_list_temp])\n",
    "\n",
    "myvnn.set_node_props(key = 'inp', node_val_zip = inp_tensor_name_to_size)\n",
    "\n",
    "# init node output sizes\n",
    "myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_inp, [5 for e in myvnn.nodes_inp]))\n",
    "myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_edge,[5 for e in myvnn.nodes_edge]))\n",
    "myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_out, [1 for e in myvnn.nodes_out]))\n",
    "\n",
    "\n",
    "# options should be controlled by node_props\n",
    "myvnn.set_node_props(key = 'flatten', node_val_zip = zip(\n",
    "    myvnn.nodes_inp, \n",
    "    [True for e in myvnn.nodes_inp]))\n",
    "\n",
    "myvnn.set_node_props(key = 'reps', node_val_zip = zip(\n",
    "    myvnn.nodes_out+myvnn.nodes_inp+myvnn.nodes_edge, \n",
    "    [1 for e in myvnn.nodes_out+myvnn.nodes_inp+myvnn.nodes_edge]))\n",
    "\n",
    "# init dropout \n",
    "myvnn.set_node_props(key = 'drop', node_val_zip = zip(\n",
    "    myvnn.nodes_out+myvnn.nodes_inp+myvnn.nodes_edge, \n",
    "    [0.0 for e in myvnn.nodes_out+myvnn.nodes_inp+myvnn.nodes_edge]))\n",
    "\n",
    "# init edge node input size (propagate forward input/edge outpus)\n",
    "myvnn.calc_edge_inp()\n",
    "\n",
    "myvnn.mk_digraph(include = ['node_name', 'inp_size', 'out_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3595fbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def reverse_edge_dict(edge_dict):\n",
    "    # Reverse connection directions. Connections are not one to one so it's not as simple as swapping keys/values\n",
    "    edge_dict_reversed = {}\n",
    "    # get new keys\n",
    "    all_values = []\n",
    "    for e in edge_dict.keys():\n",
    "        all_values+=edge_dict[e]\n",
    "    for e in set(all_values):\n",
    "        edge_dict_reversed[e] = []\n",
    "\n",
    "    for e in edge_dict.keys():\n",
    "        if edge_dict[e] == []:\n",
    "            pass\n",
    "        else:\n",
    "            for ee in edge_dict[e]: \n",
    "                edge_dict_reversed[ee] += [e]\n",
    "                # print(e, kegg_connections[e])\n",
    "                # break\n",
    "\n",
    "    # deduplicate edges\n",
    "    for e in edge_dict_reversed.keys():\n",
    "        edge_dict_reversed[e] = list(set( edge_dict_reversed[e] ))\n",
    "\n",
    "    return edge_dict_reversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bfa197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def reverse_node_props(prop_dict, \n",
    "                       conversion_dict = {'out':'inp',\n",
    "                                          'inp':'out',\n",
    "                                          'flatten':''}):\n",
    "    prop_dict_reversed = {}\n",
    "    for e in prop_dict.keys():\n",
    "        prop_dict_reversed[e] = {}\n",
    "        for ee in prop_dict[e].keys():\n",
    "            if ee not in conversion_dict.keys():\n",
    "                prop_dict_reversed[e][ee] = prop_dict[e][ee]\n",
    "            else:\n",
    "                if (conversion_dict[ee].lower() in ['none', '']):\n",
    "                    # get rid of entries liek flatten\n",
    "                    pass\n",
    "                else:\n",
    "                    prop_dict_reversed[e][conversion_dict[ee]] = prop_dict[e][ee]\n",
    "    return prop_dict_reversed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17781bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse the edges in the edge dict (and deduplicate one to many relationships)\n",
    "kegg_connections_reversed = reverse_edge_dict(edge_dict = kegg_connections)\n",
    "\n",
    "# use existing graph to define properties of new one (switch inputs/outputs)\n",
    "prop_dict_reversed = reverse_node_props(\n",
    "    prop_dict = myvnn.node_props, \n",
    "    conversion_dict = {'out':'inp',\n",
    "                       'inp':'out',\n",
    "                   'flatten':''})\n",
    "\n",
    "# use the VNNHelper class to setup the connections but then pass in all the nodes' properties directly\n",
    "myvnn_rev = VNNHelper(edge_dict = kegg_connections_reversed)\n",
    "myvnn_rev.node_props = prop_dict_reversed\n",
    "\n",
    "\n",
    "myvnn_rev.calc_edge_inp()\n",
    "myvnn_rev.mk_digraph(include = ['node_name', 'inp_size', 'out_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4338448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def Linear_block_reps(in_size, out_size, drop_pr, block_reps):\n",
    "    block_list = []\n",
    "    for i in range(block_reps):\n",
    "        if i == 0:\n",
    "            block_list += [\n",
    "                nn.Linear(in_size, out_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(drop_pr)]\n",
    "        else:\n",
    "            block_list += [\n",
    "                nn.Linear(out_size, out_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(drop_pr)]\n",
    "\n",
    "    block = nn.ModuleList(block_list)\n",
    "    return(block)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a69968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VisableNeuralNetwork(nn.Module):\n",
    "    def __init__(self, \n",
    "                 node_props, \n",
    "                 Linear_block,\n",
    "                 dependancy_order,\n",
    "                 edge_dict,\n",
    "                 node_to_inp_num_dict,\n",
    "                 **kwargs\n",
    "                ):\n",
    "        super(VisableNeuralNetwork, self).__init__()\n",
    "        self.dependancy_order = dependancy_order\n",
    "        self.edge_dict = edge_dict\n",
    "        self.node_to_inp_num_dict = node_to_inp_num_dict\n",
    "\n",
    "        # Figure out what nodes should be outputs. This will be of use for multiple predictions (yield and disease resistance say)\n",
    "        # and for VAEs.\n",
    "        all_keys = []\n",
    "        all_vals = []\n",
    "        for e in self.edge_dict.keys():\n",
    "            all_keys += [e]\n",
    "            all_vals += self.edge_dict[e]\n",
    "        all_keys = list(set(all_keys))\n",
    "        all_vals = list(set(all_vals))\n",
    "        output_names = [e for e in all_keys if e not in all_vals]\n",
    "        self.output_names = output_names\n",
    "        \n",
    "        self.return_dict = False\n",
    "        if 'return_dict' in kwargs.keys():\n",
    "            self.return_dict = kwargs['return_dict']\n",
    "\n",
    "        # Store nodes in dict\n",
    "        layer_dict = {}\n",
    "        for key in node_props.keys():\n",
    "            node_list = []\n",
    "            if 'flatten' in node_props[key]:\n",
    "                node_list += [nn.Flatten()]\n",
    "            node_list += [nn.Flatten()]\n",
    "            #TODO change linear block instead of conditioning on node name.\n",
    "            if key not in self.output_names:\n",
    "                node_list += [Linear_block(\n",
    "                    in_size=node_props[key]['inp'], \n",
    "                    out_size=node_props[key]['out'], \n",
    "                    drop_pr=node_props[key]['drop'],\n",
    "                    block_reps=node_props[key]['reps'])]\n",
    "            else:\n",
    "                node_list += [nn.Linear(node_props[key]['inp'], node_props[key]['out'])]\n",
    "\n",
    "            layer_dict[key] = nn.ModuleList(node_list)\n",
    "\n",
    "        self.layer_dict = nn.ModuleDict(layer_dict)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        temp_res_dict = {}\n",
    "        for key in self.dependancy_order:\n",
    "            \n",
    "            # if the node depends on raw inputs, get them.\n",
    "            if key in self.node_to_inp_num_dict:\n",
    "                xin = [ x[self.node_to_inp_num_dict[key]] ]\n",
    "\n",
    "            # if the node depends on inputs that have been stored in the lookup dict\n",
    "            if self.edge_dict[key] != []:\n",
    "                if xin == None:\n",
    "                    xin = []\n",
    "                xin += [temp_res_dict[e] for e in self.edge_dict[key]]\n",
    "                \n",
    "            # join all input tensors.\n",
    "            xin = torch.concat(xin, axis = 1)\n",
    "\n",
    "            for l in self.layer_dict[key]:\n",
    "                if type(l) == torch.nn.modules.container.ModuleList:\n",
    "                    for ll in l:            \n",
    "                        xin = ll(xin)\n",
    "                else:\n",
    "                    xin = l(xin)\n",
    "                    \n",
    "            temp_res_dict[key] = xin\n",
    "            xin = None \n",
    "        if (len(self.output_names) == 1) & (self.return_dict == False):\n",
    "            return temp_res_dict[self.dependancy_order[-1]]\n",
    "        else:\n",
    "            return {e:temp_res_dict[e] for e in self.output_names}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc74afaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vnn = VisableNeuralNetwork(\n",
    "    node_props = myvnn.node_props,\n",
    "    Linear_block = Linear_block_reps,\n",
    "    edge_dict = myvnn.edge_dict,\n",
    "    dependancy_order = myvnn.dependancy_order,\n",
    "    node_to_inp_num_dict = {'in1': 0, 'in2': 1},\n",
    ")\n",
    "\n",
    "model_vnn(x_list_temp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f262d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the reversed version\n",
    "model_vnn_reverse = VisableNeuralNetwork(\n",
    "    node_props = myvnn_rev.node_props,\n",
    "    Linear_block = Linear_block_reps,\n",
    "    edge_dict = myvnn_rev.edge_dict,\n",
    "    dependancy_order = myvnn_rev.dependancy_order,\n",
    "    node_to_inp_num_dict = {'y_hat': 0},\n",
    "    return_dict = True # With multiple outputs a dictionary will automatically be returned.\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    res = model_vnn_reverse([torch.zeros((200, 1))])\n",
    "    print([(e, res[e].shape) for e in list(res.keys())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a98e40",
   "metadata": {},
   "source": [
    "#### Use with multiple models (autoencoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825e973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class plVAE(pl.LightningModule):\n",
    "#     def __init__(self, vae, kl_coeff = 0.1):\n",
    "#         super().__init__()\n",
    "#         self.vae = vae\n",
    "#         self.kl_coeff = kl_coeff\n",
    "\n",
    "#     def forward(self, x, return_pqz = False):\n",
    "#         output = self.vae(x, return_pqz = return_pqz)\n",
    "#         return output\n",
    "    \n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         x = batch[0]\n",
    "#         xhat, p, q, z = self.forward(x, return_pqz = True)\n",
    "#         # calculate losses\n",
    "#         ## reconstruction\n",
    "#         recon_loss = F.mse_loss(xhat, x, reduction='mean')\n",
    "#         ## KLD\n",
    "#         log_qz = q.log_prob(z)\n",
    "#         log_pz = p.log_prob(z)\n",
    "#         kl = log_qz - log_pz\n",
    "#         kl = kl.mean()\n",
    "#         kl *= self.kl_coeff\n",
    "#         loss = kl + recon_loss\n",
    "#         self.log('train_loss', loss)\n",
    "#         return loss#, logs\n",
    "     \n",
    "#     def configure_optimizers(self, **kwargs):\n",
    "#         optimizer = torch.optim.Adam(self.parameters(), **kwargs)\n",
    "#         return optimizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91583d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c05c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [e.shape for e in x_list_temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e9e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_to_inp_num_dict = {'in1': 0, 'in2':1}\n",
    "# x_enc = model_vnn(x_list_temp)\n",
    "# x_enc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reparam\n",
    "# in_channels = 1\n",
    "# latent_channels = 2\n",
    "\n",
    "# fc_mu = nn.Sequential(nn.Linear(in_channels, latent_channels))\n",
    "# fc_log_var = nn.Sequential(nn.Linear(in_channels, latent_channels))\n",
    "\n",
    "# fc_reverse = nn.Sequential(nn.Linear(latent_channels, in_channels))\n",
    "\n",
    "# x = x_enc\n",
    "# # def forward(self, res):\n",
    "# # x = res         \n",
    "# mu = fc_mu(x)\n",
    "# log_var = fc_log_var(x)\n",
    "# # return([mu, log_var])\n",
    "\n",
    "# [e[0:3, :] for e in [mu, log_var]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8739713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sample\n",
    "# std = torch.exp(log_var/2)\n",
    "# p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "# q = torch.distributions.Normal(mu, std)\n",
    "# z = q.rsample()\n",
    "# # return p, q, z\n",
    "\n",
    "\n",
    "# # reverse parameterize\n",
    "# x = z\n",
    "# xprime = fc_reverse(x)\n",
    "# xprime[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7304a82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626652a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # x_hat_dict = model_vnn_reverse([x_enc])\n",
    "# x_hat_dict = model_vnn_reverse([xprime])\n",
    "# # convert each tensor (flattened output) to the right shape\n",
    "# for key in x_hat_dict.keys():\n",
    "#     d_b, d_l = tuple(x_hat_dict[key].shape)\n",
    "#     x_hat_dict[key] = x_hat_dict[key].reshape((d_b, 4, d_l//4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee06fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loss\n",
    "# recon_loss = torch.concat([x_hat_dict[e] - x_list_temp[node_to_inp_num_dict[e]] \n",
    "#               for e in  node_to_inp_num_dict.keys()], \n",
    "#              axis = 2).pow(2).mean()\n",
    "# # same as \n",
    "# # recon_loss = F.mse_loss(\n",
    "# #     torch.concat([x_hat_dict[e] \n",
    "# #                   for e in  node_to_inp_num_dict.keys()], axis = 2),\n",
    "# #     torch.concat([x_list_temp[node_to_inp_num_dict[e]] \n",
    "# #                   for e in  node_to_inp_num_dict.keys()], axis = 2), \n",
    "# #                   reduction='mean'\n",
    "# #                   )\n",
    "# recon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a3a2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kl_coeff = 0.1\n",
    "\n",
    "# ## KLD\n",
    "# log_qz = q.log_prob(z)\n",
    "# log_pz = p.log_prob(z)\n",
    "# kl = log_qz - log_pz\n",
    "# kl = kl.mean()\n",
    "# kl *= kl_coeff\n",
    "# loss = kl + recon_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbaf3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DemoDataset(Dataset):\n",
    "#     def __init__(self, y): self.y = y\n",
    "#     def __len__(self): return len(self.y[0])\n",
    "#     def __getitem__(self, idx): return [yy[idx] for yy in self.y]\n",
    "\n",
    "\n",
    "# training_dataloader = DataLoader(\n",
    "#     DemoDataset(y = x_list_temp),\n",
    "#     batch_size = 50, \n",
    "#     shuffle = True)\n",
    "\n",
    "# [e.shape for e in next(iter(training_dataloader))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fc2fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db2323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_hat_dict['in1'].reshape((200, 4, -1 )).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffe144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = torch.tensor([[[0, 0, 0], [0, 0, 0], [1, 1, 1]],\n",
    "#               [[2, 2, 2], [2, 2, 2], [3, 3, 3]]])\n",
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a983b5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Flatten()(y).reshape((2, 3, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd2236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6189f2e",
   "metadata": {},
   "source": [
    "### Deprecated (original) version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9f25fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building a Neural Net from an arbitrary graph\n",
    "# start by finding the top level -- all those keys which are theselves not values\n",
    "# helper function to get all keys and all value from a dict. Useful for when keys don't have unique values.\n",
    "def find_uniq_keys_values(input_dict):\n",
    "    all_keys = list(input_dict.keys())\n",
    "    all_values = []\n",
    "    for e in all_keys:\n",
    "        all_values.extend(input_dict[e])\n",
    "    all_values = list(set(all_values))\n",
    "\n",
    "    return({'all_keys': all_keys,\n",
    "           'all_values': all_values})\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40b9bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find order that nodes in the graph should be called to have all dependencies run when they are called.\n",
    "# find the dependancies for run order from many dependancies to none\n",
    "# wrapper function to find the nodes that aren't any other nodes dependancies.\n",
    "def find_top_nodes(all_key_value_dict):\n",
    "    return([e for e in all_key_value_dict['all_keys'] if e not in all_key_value_dict['all_values']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# wrapper function to find the input nodes. They don't occur in the keys and thus won't be added to the list otherwise.\n",
    "# another way to do this would have been to \n",
    "def find_input_nodes(all_key_value_dict):\n",
    "    return([e for e in all_key_value_dict['all_values'] if e not in all_key_value_dict['all_keys']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d700b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# LOD is list of dicts.\n",
    "# def kegg_brite_LOD_to_connections(\n",
    "def kegg_connections_build(\n",
    "        n_genes, \n",
    "        kegg_gene_brite):\n",
    "    \"\"\"\n",
    "    The goal here is to have a dict with each node and a list of it's children. \n",
    "    For example, the graph\n",
    "    a--b--d\n",
    "    |-c--e\n",
    "    Would be parsed into     \n",
    "    {'a':['b', 'c'],\n",
    "    'b':['d'],\n",
    "    'c':['e']}\n",
    "    \"\"\"\n",
    "    kegg_connections = {}\n",
    "    # for all genes in list\n",
    "    for i in tqdm(range(n_genes)): \n",
    "        temp = kegg_gene_brite[i]['BRITE']['BRITE_PATHS']\n",
    "        # clean up to make sure that there are no \":\" characters. These can mess up graphviz\n",
    "        temp = [[temp[j][i].replace(':', '-') for i in range(len(temp[j])) ] for j in range(len(temp))]\n",
    "        # all paths through graph associated with a gene\n",
    "        for j in range(len(temp)):\n",
    "            # steps of the path through the graph\n",
    "            for k in range(len(temp[j])-1):\n",
    "                \n",
    "                # name standardization \n",
    "                temp_jk  = temp[j][k]\n",
    "                temp_jk1 = temp[j][k+1]\n",
    "                temp_jk  = temp_jk.lower().title().replace(' ', '')\n",
    "                temp_jk1 = temp_jk1.lower().title().replace(' ', '')\n",
    "                \n",
    "                # if this is a new key, add it and add the k+1 entry as it's child\n",
    "                if temp_jk  not in kegg_connections.keys():\n",
    "                    kegg_connections[temp_jk] = [temp_jk1]\n",
    "                else: \n",
    "                    # Check to see if there's a new child to add   \n",
    "                    if temp_jk1 not in kegg_connections[temp_jk]:\n",
    "                        # make sure that no key contains itself. This was a problem for 'Others' which is now disallowed.\n",
    "                        if (temp_jk != temp_jk1):\n",
    "                            # add it.\n",
    "                            kegg_connections[temp_jk].extend([temp_jk1])\n",
    "    return(kegg_connections)          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602f3b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def kegg_connections_clean(kegg_connections):\n",
    "    if 'Others' in kegg_connections.keys():\n",
    "        del kegg_connections['Others']\n",
    "        print('Removed node \"Others\"')\n",
    "\n",
    "    # remove 'Others' as a possible value\n",
    "    for key in kegg_connections.keys():\n",
    "        kegg_connections[key] = [e for e in kegg_connections[key] if e != 'Others']\n",
    "\n",
    "    # Make sure that no list contains it's own key\n",
    "    for key in kegg_connections.keys():\n",
    "        kegg_connections[key] = [e for e in kegg_connections[key] if e != key]\n",
    "\n",
    "    # there might be associations with no dependants and with no dependants except those that have no dependants.\n",
    "    # Build up a list with those keys that don't connect back to snps then I'll pass over the connection dict once to remove references to them.\n",
    "    rm_list = []\n",
    "    rm_list_i = len(rm_list)\n",
    "    rm_list_j = -1\n",
    "    for i in range(100):\n",
    "        if rm_list_i == rm_list_j:\n",
    "            break\n",
    "        else:\n",
    "            rm_list = [key for key in kegg_connections.keys() if [e for e in kegg_connections[key] if e not in rm_list] == []]\n",
    "            rm_list_j = rm_list_i \n",
    "            rm_list_i = len(rm_list)\n",
    "    # rm_list\n",
    "\n",
    "    for key in rm_list:\n",
    "        del kegg_connections[key]\n",
    "        \n",
    "    for key in kegg_connections.keys():\n",
    "        kegg_connections[key] = [e for e in kegg_connections[key] if e not in rm_list]\n",
    "    return kegg_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be1d703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def kegg_connections_append_y_hat(kegg_connections):\n",
    "    # add yhat node to the graph\n",
    "    temp_values = []\n",
    "    for key in kegg_connections.keys():\n",
    "        temp_values += kegg_connections[key]\n",
    "\n",
    "    kegg_connections['y_hat'] = [key for key in kegg_connections.keys() if key not in temp_values]\n",
    "    return kegg_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90b05ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kegg_connections_digraph(kegg_connections, option = ''):\n",
    "    dot = ''\n",
    "    if option == '':\n",
    "        dot = Digraph()\n",
    "        for key in tqdm(kegg_connections.keys()):\n",
    "            dot.node(key)\n",
    "            for value in kegg_connections[key]:\n",
    "                # edge takes a head/tail whereas edges takes name pairs concatednated (A, B -> AB)in a list\n",
    "                dot.edge(value, key)\n",
    "\n",
    "    if option == 'number':\n",
    "        name_to_num_dict = dict(zip(list(kegg_connections.keys()),\n",
    "                                    [str(i) for i in range(len(list(kegg_connections.keys())))]))\n",
    "\n",
    "        temp = {}\n",
    "        for key in kegg_connections.keys():\n",
    "            temp[name_to_num_dict[key]] = [name_to_num_dict[e] if e in name_to_num_dict.keys() else e for e in kegg_connections[key]]\n",
    "\n",
    "        dot = Digraph()\n",
    "        for key in tqdm(temp.keys()):\n",
    "            dot.node(key)\n",
    "            for value in temp[key]:\n",
    "                # edge takes a head/tail whereas edges takes name pairs concatednated (A, B -> AB)in a list\n",
    "                dot.edge(value, key)                 \n",
    "                 \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c90100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kegg_connections_find_in_out_nodes(kegg_connections):\n",
    "    # start by finding the top level -- all those keys which are theselves not values\n",
    "    res = find_uniq_keys_values(input_dict = kegg_connections)\n",
    "    all_keys = res['all_keys']\n",
    "    all_values = res['all_values']\n",
    "\n",
    "    # use the keys to find the input/outputs of the graph\n",
    "    output_nodes = [e for e in all_keys if e not in all_values]\n",
    "    input_nodes = [e for e in all_values if e not in all_keys]\n",
    "    return input_nodes, output_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc127307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kegg_connections_find_dependancy_order(kegg_connections, input_nodes):\n",
    "    # find the dependancies for run order from many dependancies to none\n",
    "    temp = kegg_connections.copy()\n",
    "\n",
    "    no_dependants = find_input_nodes(all_key_value_dict = find_uniq_keys_values(input_dict = temp))\n",
    "    # first pass. Same as the output nodes identified above\n",
    "    dependancy_order = []\n",
    "    # Then iterate\n",
    "    for ith in range(100): #TODO <- this should be set as a input parameter\n",
    "        top_nodes = find_top_nodes(all_key_value_dict = find_uniq_keys_values(input_dict = temp))\n",
    "        if top_nodes == []:\n",
    "            break\n",
    "        else:\n",
    "            dependancy_order += top_nodes    \n",
    "            # remove nodes from the graph that are at the 'top' level and haven't already been removed\n",
    "            for key in [e for e in dependancy_order if e in temp.keys()]:\n",
    "                temp.pop(key)\n",
    "\n",
    "    # reverse to get the order that the nodes should be called\n",
    "    dependancy_order.reverse()\n",
    "    # dependancy_order\n",
    "\n",
    "    # Trying out new approach: add a node for the input data tha will only flatten the input.\n",
    "    dependancy_order = input_nodes+dependancy_order\n",
    "\n",
    "    for key in input_nodes:\n",
    "        kegg_connections[key] = [] #[key] # needs to contain itself so the model's `get_input_node()` function works \n",
    "                                # or that function needs to change.\n",
    "    \n",
    "    return kegg_connections, dependancy_order, no_dependants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7573fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kegg_gene_brite_build_node_lookup_dict(kegg_gene_brite):\n",
    "    # build a dict to go from the node names in `no_dependants` to the list index in `ACGT_gene_slice_list`\n",
    "    brite_node_to_list_idx_dict = {}\n",
    "    for i in tqdm(range(len(kegg_gene_brite))):\n",
    "        brite_node_to_list_idx_dict[str(kegg_gene_brite[i]['BRITE']['BRITE_PATHS'][0][-1])] = i\n",
    "    return brite_node_to_list_idx_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b97971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_tensor_dict(ACGT_gene_slice_list, no_dependants, brite_node_to_list_idx_dict):    \n",
    "    input_tensor_dict = {}\n",
    "    for e in no_dependants:\n",
    "        input_tensor_dict[e] = ACGT_gene_slice_list[brite_node_to_list_idx_dict[e]]\n",
    "    \n",
    "    return input_tensor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c89da36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_network_param_dicts(\n",
    "        kegg_connections,  # for input sizes\n",
    "        input_tensor_dict, # for input sizes\n",
    "        dependancy_order,\n",
    "        default_output_size,\n",
    "        default_dropout_pr,\n",
    "        default_block_reps,\n",
    "    ):\n",
    "    # Figure out expected input/output shapes\n",
    "    #==NOTE! This assumes only dense connections!==\n",
    "\n",
    "    # This could be replaced by a sort of \"distance from output\" measure\n",
    "    output_size_dict = dict(zip(dependancy_order, \n",
    "                            [default_output_size for i in range(len(dependancy_order))]))\n",
    "    output_size_dict['y_hat'] = 1 \n",
    "\n",
    "\n",
    "    # Setup dropout % dictionary\n",
    "    dropout_pr_dict = dict(zip(dependancy_order, \n",
    "                            [default_dropout_pr for i in range(len(dependancy_order))]))\n",
    "    dropout_pr_dict['y_hat'] = 0 # not required, output node is purely linear without dropout\n",
    "\n",
    "\n",
    "    # Setup replicates of layers dictionary\n",
    "    block_rep_dict = dict(zip(dependancy_order, \n",
    "                            [default_block_reps for i in range(len(dependancy_order))]))\n",
    "    block_rep_dict['y_hat'] = 1 # not required, output node is purely linear. Not a linear block\n",
    "\n",
    "    # output_size_dict\n",
    "    # dropout_pr_dict\n",
    "    # block_rep_dict\n",
    "\n",
    "    # CHANNEL AWARE VERSION -----------------------------------------------------------------------------------\n",
    "    input_size_dict = kegg_connections.copy()\n",
    "\n",
    "    # use the expected output sizes from `output_size_dict` to fill in the non-data sizes\n",
    "    tensor_ndim = len(input_tensor_dict[list(input_tensor_dict.keys())[0]].shape)\n",
    "    for e in tqdm(input_size_dict.keys()):\n",
    "        # overwrite named connections with the output size of those connections\n",
    "        # if the entry is in no_dependants it's data so it's size needs to be grabbed from the input_tensor_dict\n",
    "        \n",
    "        # is there no channel dim? (major/minor allele)\n",
    "        if 2 == tensor_ndim:\n",
    "            input_size_dict[e] = [\n",
    "                list(input_tensor_dict[ee].shape)[-1] # <- NOTE! THIS ASSUMES ONLY DENSE CONNECTIONS (i.e. only the 1st dim is needed)\n",
    "                if ee in no_dependants\n",
    "                else output_size_dict[ee] for ee in input_size_dict[e]]\n",
    "        elif 3 == tensor_ndim: # There is a channel dim\n",
    "            input_size_dict[e] = [\n",
    "                (list(input_tensor_dict[ee].shape)[1]*list(input_tensor_dict[ee].shape)[2]) # <- NOTE! THIS ASSUMES ONLY DENSE CONNECTIONS (i.e. only the 1st dim is needed)  \n",
    "                if ee in no_dependants\n",
    "                else output_size_dict[ee] for ee in input_size_dict[e]]\n",
    "\n",
    "    # Now walk over entries and overwrite with the sum of the inputs\n",
    "    for e in tqdm(input_size_dict.keys()):\n",
    "        input_size_dict[e] = np.sum(input_size_dict[e])\n",
    "\n",
    "    return input_size_dict, output_size_dict, dropout_pr_dict, block_rep_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519263fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from EnvDL.dlfn import g2fc_datawrapper, BigDataset, plDNN_general\n",
    "\n",
    "default_output_size = 2\n",
    "default_dropout_pr = 0.0\n",
    "default_block_reps = 1\n",
    "\n",
    "\n",
    "X = g2fc_datawrapper()\n",
    "X.set_split()\n",
    "X.load_all(name_list = ['obs_geno_lookup', 'YMat', 'KEGG_slices',], store=True) \n",
    "X.calc_cs('YMat', version = 'np', filter = 'val:train')\n",
    "ACGT_gene_slice_list =     X.get('KEGG_slices', ops_string='')\n",
    "parsed_kegg_gene_entries = X.get('KEGG_entries')\n",
    "\n",
    "\n",
    "# Restrict to only those with pathway\n",
    "kegg_gene_brite = [e for e in parsed_kegg_gene_entries if 'BRITE' in e.keys()]\n",
    "\n",
    "# also require to have a non-empty path\n",
    "kegg_gene_brite = [e for e in kegg_gene_brite if not e['BRITE']['BRITE_PATHS'] == []]\n",
    "\n",
    "print('Retaining '+ str(round(len(kegg_gene_brite)/len(parsed_kegg_gene_entries), 4)*100)+'%, '+str(len(kegg_gene_brite)\n",
    "    )+'/'+str(len(parsed_kegg_gene_entries)\n",
    "    )+' Entries'\n",
    "    )\n",
    "# kegg_gene_brite[1]['BRITE']['BRITE_PATHS']\n",
    "\n",
    "\n",
    "kegg_connections = kegg_connections_append_y_hat(\n",
    "    kegg_connections = kegg_connections_clean(\n",
    "        kegg_connections = kegg_connections_build(\n",
    "            n_genes = 2, # 6067, \n",
    "            kegg_gene_brite = kegg_gene_brite) ) )\n",
    "\n",
    "len(list(kegg_connections.keys()))\n",
    "\n",
    "\n",
    "input_nodes, output_nodes                         = kegg_connections_find_in_out_nodes(kegg_connections= kegg_connections)\n",
    "kegg_connections, dependancy_order, no_dependants = kegg_connections_find_dependancy_order(kegg_connections= kegg_connections, input_nodes= input_nodes)\n",
    "brite_node_to_list_idx_dict                       = kegg_gene_brite_build_node_lookup_dict(kegg_gene_brite = kegg_gene_brite)\n",
    "input_tensor_dict                                 = build_input_tensor_dict(ACGT_gene_slice_list=ACGT_gene_slice_list, no_dependants=no_dependants, brite_node_to_list_idx_dict = brite_node_to_list_idx_dict)\n",
    "\n",
    "input_size_dict, output_size_dict, dropout_pr_dict, block_rep_dict = setup_network_param_dicts(\n",
    "        kegg_connections = kegg_connections,\n",
    "        input_tensor_dict = input_tensor_dict,\n",
    "        dependancy_order = dependancy_order,\n",
    "        default_output_size = default_output_size,\n",
    "        default_dropout_pr = default_dropout_pr,\n",
    "        default_block_reps = default_block_reps,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18128f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make in out size graph\n",
    "dot = ''\n",
    "dot = Digraph()\n",
    "for key in tqdm(kegg_connections.keys()):\n",
    "    key_label = 'in: '+str(input_size_dict[key])+'\\nout: '+str(output_size_dict[key])\n",
    "    dot.node(key, key_label)\n",
    "    for value in kegg_connections[key\n",
    "]:\n",
    "        # edge takes a head/tail whereas edges takes name pairs concatednated (A, B -> AB)in a list\n",
    "        dot.edge(value, key)    \n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfb52bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working version ====\n",
    "# Doesn't pass output node through relu\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, \n",
    "                 example_dict, # contains the node (excluding input tensors)\n",
    "                 example_dict_input_size, # contains the input sizes (including the tensors)\n",
    "                 example_dict_output_size,\n",
    "                 example_dict_dropout_pr,\n",
    "                 example_block_rep_dict,\n",
    "                 input_tensor_names,\n",
    "                 dependancy_order\n",
    "                ):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        def Linear_block_reps(in_size, out_size, drop_pr, block_reps):\n",
    "            block_list = []\n",
    "            for i in range(block_reps):\n",
    "                if i == 0:\n",
    "                    block_list += [\n",
    "                        nn.Linear(in_size, out_size),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(drop_pr)]\n",
    "                else:\n",
    "                    block_list += [\n",
    "                        nn.Linear(out_size, out_size),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(drop_pr)]\n",
    "        \n",
    "            block = nn.ModuleList(block_list)\n",
    "            return(block)           \n",
    "        \n",
    "        # fill in the list in dependancy order. \n",
    "        layer_list = []\n",
    "        for key in dependancy_order:\n",
    "            if key in input_tensor_names:\n",
    "                layer_list += [\n",
    "                    nn.Flatten()\n",
    "                ]\n",
    "            elif key != 'y_hat':\n",
    "                layer_list += [\n",
    "                    Linear_block_reps(in_size=example_dict_input_size[key], \n",
    "                                 out_size=example_dict_output_size[key], \n",
    "                                 drop_pr=example_dict_dropout_pr[key],\n",
    "                                 block_reps=example_block_rep_dict[key])\n",
    "                              ]\n",
    "            else:\n",
    "                layer_list += [\n",
    "                    nn.Linear(example_dict_input_size[key], \n",
    "                              example_dict_output_size[key])\n",
    "                              ]\n",
    "                \n",
    "\n",
    "        self.nn_layer_list = nn.ModuleList(layer_list)\n",
    "\n",
    "        # things for get_input_node in forward to work.\n",
    "        self.example_dict = example_dict\n",
    "        self.input_tensor_names = input_tensor_names\n",
    "        self.dependancy_order = dependancy_order\n",
    "        \n",
    "        self.input_tensor_lookup = dict(zip(input_tensor_names, \n",
    "                                            [i for i in range(len(input_tensor_names))]))\n",
    "        self.result_list = []\n",
    "        self.result_list_lookup = {}\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Note: x will be a list. input_tensor_lookup will contain the name: list index pairs.\n",
    "        # I use a dict instead of a list comprehension here because there could be an arbitrarily\n",
    "        # large number of inputs in the list. \n",
    "        def get_input_node(self, input_node, get_x):  \n",
    "#             print(input_node, self.result_list_lookup)\n",
    "            return(self.result_list[self.result_list_lookup[input_node]])\n",
    "        \n",
    "        # trying reinstantiating to get around inplace replacement issue.\n",
    "        self.result_list = []\n",
    "        self.result_list_lookup = {}\n",
    "        for key in self.dependancy_order:\n",
    "            input_nodes = self.example_dict[key]\n",
    "            nn_layer_list_idx = [i for i in range(len(dependancy_order)) if dependancy_order[i]==key][0]\n",
    "            \n",
    "            self.result_list_lookup[key] = len(self.result_list_lookup)                \n",
    "            if key in self.input_tensor_names: # If the input node is an input (flatten) layer\n",
    "                self.result_list = self.result_list + [self.nn_layer_list[nn_layer_list_idx](\n",
    "                    x[self.input_tensor_lookup[key]]\n",
    "                ).clone()]\n",
    "\n",
    "            elif key != 'y_hat':\n",
    "                # refactored to handle module lists (even if module list contains only one entry)\n",
    "                out = torch.concat(\n",
    "                    [get_input_node(self, input_node = e, get_x = x) for e in input_nodes], \n",
    "                    -1)\n",
    "            \n",
    "                for module in self.nn_layer_list[nn_layer_list_idx]:\n",
    "                    out = module(out)\n",
    "        \n",
    "                self.result_list = self.result_list + [out] \n",
    "            \n",
    "            else:\n",
    "                self.result_list = self.result_list + [self.nn_layer_list[nn_layer_list_idx](torch.concat(\n",
    "                    [get_input_node(self, input_node = e, get_x = x) for e in input_nodes], \n",
    "                    -1)).clone()]            \n",
    "\n",
    "        return self.result_list[self.result_list_lookup['y_hat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dd5713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ListDataset(Dataset): # for any G containing matix with many (phno) to one (geno)\n",
    "    def __init__(self, \n",
    "                 y, \n",
    "                 x_list,\n",
    "                 obs_idxs, # this is a list of the indexes used. It allows us to pass in smaller \n",
    "                           # tensors and then get the right genotype\n",
    "                 obs_geno_lookup,\n",
    "                 transform = None, target_transform = None,\n",
    "                 **kwargs \n",
    "                ):\n",
    "        if 'device' in kwargs:\n",
    "            self.device = kwargs['device']\n",
    "        self.y = y \n",
    "        self.x_list = x_list\n",
    "        self.obs_idxs = obs_idxs\n",
    "        self.obs_geno_lookup = obs_geno_lookup\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        y_idx =self.y[idx]\n",
    "        \n",
    "        new_idx = self.obs_idxs[idx]\n",
    "        idx_geno = self.obs_geno_lookup[new_idx, 1]\n",
    "        x_idx =[x[idx_geno, ] for x in self.x_list] \n",
    "        \n",
    "        if self.target_transform:\n",
    "            y_idx = self.transform(y_idx)\n",
    "            x_idx = [self.transform(x) for x in x_idx]\n",
    "            \n",
    "        return y_idx, x_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605c70ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class plVNN(pl.LightningModule):\n",
    "    def __init__(self, mod):\n",
    "        super().__init__()\n",
    "        self.mod = mod\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_i, xs_i = batch\n",
    "        pred = self.mod(xs_i)\n",
    "        loss = F.mse_loss(pred, y_i)\n",
    "        self.log(\"train_loss\", loss)   \n",
    "        return(loss)\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_i, xs_i = batch\n",
    "        pred = self.mod(xs_i)\n",
    "        loss = F.mse_loss(pred, y_i)\n",
    "        self.log('val_loss', loss)        \n",
    "     \n",
    "    def configure_optimizers(self, **kwargs):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), **kwargs)\n",
    "        return optimizer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f6755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True == False:\n",
    "    x_list_temp = [torch.from_numpy(input_tensor_dict[key]).to(torch.float) for key in input_tensor_dict.keys()]\n",
    "\n",
    "\n",
    "    training_dataloader = DataLoader(ListDataset(\n",
    "            y =               X.get('YMat',ops_string='cs filter:val:train asarray from_numpy float cuda:0')[:, None],\n",
    "            x_list = [e.to('cuda') for e in x_list_temp],\n",
    "            obs_idxs =        X.get('val:train',       ops_string='   asarray from_numpy      '), \n",
    "            obs_geno_lookup = X.get('obs_geno_lookup', ops_string='   asarray from_numpy      ')\n",
    "        ),\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True\n",
    "    )\n",
    "\n",
    "\n",
    "    validation_dataloader = DataLoader(ListDataset(\n",
    "            y =               X.get('YMat',ops_string='cs filter:val:test asarray from_numpy float cuda:0')[:, None],\n",
    "            x_list = [e.to('cuda') for e in x_list_temp],\n",
    "            obs_idxs =        X.get('val:test',       ops_string='   asarray from_numpy      '), \n",
    "            obs_geno_lookup = X.get('obs_geno_lookup', ops_string='   asarray from_numpy      ')\n",
    "        ),\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True\n",
    "    )\n",
    "\n",
    "    model = NeuralNetwork(example_dict = kegg_connections, \n",
    "                        example_dict_input_size = input_size_dict,\n",
    "                        example_dict_output_size = output_size_dict,\n",
    "                        example_dict_dropout_pr= dropout_pr_dict,\n",
    "                        example_block_rep_dict = block_rep_dict,\n",
    "                        input_tensor_names = list(input_tensor_dict.keys()),\n",
    "                        dependancy_order = dependancy_order)\n",
    "\n",
    "\n",
    "    model.to('cuda')\n",
    "    # LSUV_(model, data = next(iter(training_dataloader))[1])\n",
    "    print(model(next(iter(training_dataloader))[1])[0:5])\n",
    "    # print(next(model.parameters()))\n",
    "    # print(model)\n",
    "\n",
    "    \n",
    "    VNN = plVNN(model)\n",
    "    optimizer = VNN.configure_optimizers()\n",
    "\n",
    "    logger = TensorBoardLogger(\"tb_vnn_logs\", name=\"vnn-02.31-TESTING-REMOVE-ME\")\n",
    "    trainer = pl.Trainer(max_epochs=max_epoch, logger=logger, precision=16)\n",
    "\n",
    "    trainer.fit(model=VNN, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782b8ca1",
   "metadata": {},
   "source": [
    "### Alternate version (using VNNHelper) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a63179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same setup as above to create kegg_gene_brite\n",
    "X = g2fc_datawrapper()\n",
    "X.set_split()\n",
    "X.load_all(name_list = ['obs_geno_lookup', 'YMat', 'KEGG_slices',], store=True) \n",
    "X.calc_cs('YMat', version = 'np', filter = 'val:train')\n",
    "ACGT_gene_slice_list =     X.get('KEGG_slices', ops_string='')\n",
    "parsed_kegg_gene_entries = X.get('KEGG_entries')\n",
    "\n",
    "\n",
    "# Restrict to only those with pathway\n",
    "kegg_gene_brite = [e for e in parsed_kegg_gene_entries if 'BRITE' in e.keys()]\n",
    "\n",
    "# also require to have a non-empty path\n",
    "kegg_gene_brite = [e for e in kegg_gene_brite if not e['BRITE']['BRITE_PATHS'] == []]\n",
    "\n",
    "print('Retaining '+ str(round(len(kegg_gene_brite)/len(parsed_kegg_gene_entries), 4)*100)+'%, '+str(len(kegg_gene_brite)\n",
    "    )+'/'+str(len(parsed_kegg_gene_entries)\n",
    "    )+' Entries'\n",
    "    )\n",
    "# kegg_gene_brite[1]['BRITE']['BRITE_PATHS']\n",
    "\n",
    "\n",
    "kegg_connections = kegg_connections_append_y_hat(\n",
    "    kegg_connections = kegg_connections_clean(\n",
    "        kegg_connections = kegg_connections_build(\n",
    "            n_genes = 4, #6067, \n",
    "            kegg_gene_brite = kegg_gene_brite) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1caa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def kegg_connections_sanitize_names(kegg_connections, replace_chars = {'.':'_'}):\n",
    "    # have to clean the key values so that none have '.' because I'm looking up nodes by name and pytorch doesn't allow this.\n",
    "    #NOTE this will potentially (but not here) create a bug if there are genes named with '.'    \n",
    "\n",
    "    def replace_select_chars(in_str, replace_chars):\n",
    "        for key in replace_chars.keys():\n",
    "            in_str = in_str.replace(key, replace_chars[key])\n",
    "        return in_str\n",
    "\n",
    "    new_kegg_connections = {}\n",
    "    for key in kegg_connections.keys():\n",
    "        new_kegg_connections[replace_select_chars(in_str = key, replace_chars=replace_chars)] = [replace_select_chars(in_str = e, replace_chars=replace_chars) for e in kegg_connections[key]]\n",
    "\n",
    "    return new_kegg_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd655af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kegg_connections = kegg_connections_sanitize_names(\n",
    "    kegg_connections = kegg_connections, \n",
    "    replace_chars = {'.':'_'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8137b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allowed boilerplate.\n",
    "\n",
    "# initialize helper for input nodes\n",
    "myvnn = VNNHelper(edge_dict = kegg_connections)\n",
    "\n",
    "myvnn.nodes_inp[0:10]\n",
    "\n",
    "# Get a mapping of brite names to tensor list index\n",
    "find_names = myvnn.nodes_inp # e.g. ['100383860', '100278565', ... ]\n",
    "lookup_dict = {}\n",
    "\n",
    "# the only difference lookup_dict and brite_node_to_list_idx_dict above is that this is made using the full set of genes in the list \n",
    "# whereas that is made using kegg_gene_brite which is a subset\n",
    "for i in range(len(parsed_kegg_gene_entries)):\n",
    "    if 'BRITE' not in parsed_kegg_gene_entries[i].keys():\n",
    "        pass\n",
    "    elif parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'] == []:\n",
    "        pass\n",
    "    else:\n",
    "        name = parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'][0][-1]\n",
    "        if name in find_names:\n",
    "            lookup_dict[name] = i\n",
    "lookup_dict    \n",
    "\n",
    "\n",
    "brite_node_to_list_idx_dict = {}\n",
    "for i in tqdm(range(len(kegg_gene_brite))):\n",
    "    brite_node_to_list_idx_dict[str(kegg_gene_brite[i]['BRITE']['BRITE_PATHS'][0][-1])] = i        \n",
    "\n",
    "# Get the input sizes for the graph\n",
    "size_in_zip = zip(myvnn.nodes_inp, [np.prod(ACGT_gene_slice_list[lookup_dict[e]].shape[1:]) for e  in myvnn.nodes_inp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057f71a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init input node sizes\n",
    "myvnn.set_node_props(key = 'inp', node_val_zip = size_in_zip)\n",
    "\n",
    "# # init node output sizes\n",
    "myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_inp, [5 for e in myvnn.nodes_inp]))\n",
    "myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_edge,[5 for e in myvnn.nodes_edge]))\n",
    "myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_out, [1 for e in myvnn.nodes_out]))\n",
    "\n",
    "\n",
    "# # options should be controlled by node_props\n",
    "myvnn.set_node_props(key = 'flatten', node_val_zip = zip(\n",
    "    myvnn.nodes_inp, \n",
    "    [True for e in myvnn.nodes_inp]))\n",
    "\n",
    "myvnn.set_node_props(key = 'reps', node_val_zip = zip(\n",
    "    myvnn.nodes_out+myvnn.nodes_inp+myvnn.nodes_edge, \n",
    "    [1 for e in myvnn.nodes_out+myvnn.nodes_inp+myvnn.nodes_edge]))\n",
    "\n",
    "# init dropout \n",
    "myvnn.set_node_props(key = 'drop', node_val_zip = zip(\n",
    "    myvnn.nodes_out+myvnn.nodes_inp+myvnn.nodes_edge, \n",
    "    [0.0 for e in myvnn.nodes_out+myvnn.nodes_inp+myvnn.nodes_edge]))\n",
    "\n",
    "# init edge node input size (propagate forward input/edge outpus)\n",
    "myvnn.calc_edge_inp()\n",
    "\n",
    "myvnn.mk_digraph(include = ['node_name', 'inp_size', 'out_size'])\n",
    "# myvnn.mk_digraph(include = [''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeee2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True == False:\n",
    "    model = VisableNeuralNetwork(\n",
    "        node_props = myvnn.node_props,\n",
    "        Linear_block = Linear_block_reps,\n",
    "        edge_dict = myvnn.edge_dict,\n",
    "        dependancy_order = myvnn.dependancy_order,\n",
    "\n",
    "        node_to_inp_num_dict = lookup_dict\n",
    "    )\n",
    "    model = model.to('cuda')\n",
    "    # model.cuda()\n",
    "    next(model.parameters()).is_cuda\n",
    "\n",
    "    model(\n",
    "        # selectively send the tensors I need to the GPU, replace all others with None. (so it if it fails it fails fast and loudly)\n",
    "        [torch.tensor(ACGT_gene_slice_list[i]).to(torch.float).to('cuda') if \n",
    "        i in [lookup_dict[e] for e in lookup_dict.keys()] # indices to send to gpu\n",
    "        else None\n",
    "        for i in range(len(ACGT_gene_slice_list))]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792c94e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example where only relevant data is moved to the GPU\n",
    "if True == False:\n",
    "    batch_size = 24\n",
    "\n",
    "    sparce_list = [\n",
    "        # selectively send the tensors I need to the GPU, replace all others with None. (so it if it fails it fails fast and loudly)\n",
    "        torch.tensor(ACGT_gene_slice_list[i]).to(torch.float).to('cuda') if \n",
    "        i in [lookup_dict[e] for e in lookup_dict.keys()] # indices to send to gpu\n",
    "        else torch.tensor(ACGT_gene_slice_list[i]).to(torch.float) # keep on cpu (not needed)\n",
    "        for i in range(len(ACGT_gene_slice_list))]\n",
    "\n",
    "\n",
    "    training_dataloader = DataLoader(ListDataset(\n",
    "            y =               X.get('YMat',ops_string='cs filter:val:train asarray from_numpy float cuda:0')[:, None],\n",
    "            x_list = sparce_list,\n",
    "            obs_idxs =        X.get('val:train',       ops_string='   asarray from_numpy      '), \n",
    "            obs_geno_lookup = X.get('obs_geno_lookup', ops_string='   asarray from_numpy      ')\n",
    "        ),\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True\n",
    "    )\n",
    "\n",
    "\n",
    "    validation_dataloader = DataLoader(ListDataset(\n",
    "            y =               X.get('YMat',ops_string='cs filter:val:test asarray from_numpy float cuda:0')[:, None],\n",
    "            x_list = sparce_list,\n",
    "            obs_idxs =        X.get('val:test',        ops_string='   asarray from_numpy      '), \n",
    "            obs_geno_lookup = X.get('obs_geno_lookup', ops_string='   asarray from_numpy      ')\n",
    "        ),\n",
    "        batch_size = batch_size,\n",
    "        shuffle = False\n",
    "    )\n",
    "\n",
    "    VNN = plVNN(model)\n",
    "    optimizer = VNN.configure_optimizers()\n",
    "\n",
    "    logger = TensorBoardLogger(\"tb_vnn_logs\", name=\"vnn-02.31-TESTING-REMOVE-ME\")\n",
    "    trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "\n",
    "    trainer.fit(model=VNN, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b30e06e",
   "metadata": {},
   "source": [
    "#### Extend VNN to VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f479bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_VNN_VAE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4b3f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VNNVAEHelper(nn.Module):\n",
    "    def __init__(self, encoder, decoder, latent_inp_size, latent_size):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.fc_mu      = nn.Sequential(nn.Linear(latent_inp_size, latent_size))\n",
    "        self.fc_log_var = nn.Sequential(nn.Linear(latent_inp_size, latent_size))\n",
    "        self.fc_reverse = nn.Sequential(nn.Linear(latent_size, latent_inp_size))\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def reparam(self, x):\n",
    "        mu      = self.fc_mu(x)\n",
    "        log_var = self.fc_log_var(x)\n",
    "        return([mu, log_var])\n",
    "    \n",
    "    def sample(self, mu, log_var):\n",
    "        std = torch.exp(log_var/2)\n",
    "        p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "        z = q.rsample()\n",
    "        return p, q, z\n",
    "    \n",
    "    def reparam_rev(self, z):\n",
    "        return self.fc_reverse(z)\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def forward(self, x, return_encoding = False):\n",
    "        x_enc       = self.encode(x=x)\n",
    "        if not return_encoding:\n",
    "            mu, log_var = self.reparam(x=x_enc)\n",
    "            p, q, z     = self.sample(mu=mu, log_var=log_var)\n",
    "            xprime      = self.reparam_rev(z=z)\n",
    "            xhat        = self.decode([xprime])\n",
    "\n",
    "        if return_encoding: return x_enc\n",
    "        else: return xhat, p, q, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab54ddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class plVNNVAE(pl.LightningModule):\n",
    "    def __init__(self, vvh, lookup_dict, kl_coeff = 0.1):\n",
    "        super().__init__()\n",
    "        self.VNNVAEHelper = vvh\n",
    "        self.kl_coeff = kl_coeff\n",
    "        self.lookup_dict = lookup_dict\n",
    "\n",
    "    def forward(self, x, return_encoding = False):\n",
    "        return self.VNNVAEHelper(x, return_encoding = return_encoding)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[1]\n",
    "        xhat, p, q, z = self.forward(x, return_encoding = False)\n",
    "        # calculate losses\n",
    "        ## reconstruction\n",
    "        tensor_keys = tuple(self.lookup_dict.keys())\n",
    "\n",
    "        recon_loss = F.mse_loss(\n",
    "            torch.concat([xhat[e]                      for e in tensor_keys], axis = 1), \n",
    "            torch.concat([x[lookup_dict[e]].flatten(1) for e in tensor_keys], axis = 1),        \n",
    "            reduction='mean')\n",
    "\n",
    "        # recon_loss = F.mse_loss(xhat, x, reduction='mean')\n",
    "        ## KLD\n",
    "        log_qz = q.log_prob(z)\n",
    "        log_pz = p.log_prob(z)\n",
    "        kl = log_qz - log_pz\n",
    "        kl = kl.mean()\n",
    "        kl *= self.kl_coeff\n",
    "        loss = kl + recon_loss\n",
    "        self.log('train_loss', loss)\n",
    "        return loss#, logs\n",
    "     \n",
    "    def configure_optimizers(self, **kwargs):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), **kwargs)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac2d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if demo_VNN_VAE:\n",
    "    batch_size = 24\n",
    "\n",
    "    sparce_list = [torch.tensor(e).to(torch.float) for e in ACGT_gene_slice_list]\n",
    "    # use deduplicated list of genotypes because I'm ignoring y\n",
    "    deduplicated_obs_wrt_g = torch.from_numpy(np.asarray(list(set(X.get('obs_geno_lookup', ops_string='')[:, 2]))))\n",
    "\n",
    "    training_dataloader = DataLoader(ListDataset(\n",
    "            y               = deduplicated_obs_wrt_g[:, None],\n",
    "            x_list          = sparce_list,\n",
    "            obs_idxs        = deduplicated_obs_wrt_g, \n",
    "            obs_geno_lookup = X.get('obs_geno_lookup', ops_string='asarray from_numpy')\n",
    "        ),\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True\n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(ListDataset(\n",
    "            y               = deduplicated_obs_wrt_g[:, None],\n",
    "            x_list          = sparce_list,\n",
    "            obs_idxs        = deduplicated_obs_wrt_g, \n",
    "            obs_geno_lookup = X.get('obs_geno_lookup', ops_string='asarray from_numpy')\n",
    "        ),\n",
    "        batch_size = batch_size,\n",
    "        shuffle = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673c1d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "if demo_VNN_VAE: # Setup VNNHelper for Encoder \n",
    "    kegg_connections = kegg_connections_sanitize_names(\n",
    "        kegg_connections = kegg_connections_append_y_hat(\n",
    "            kegg_connections = kegg_connections_clean(\n",
    "                kegg_connections = kegg_connections_build(\n",
    "                    n_genes = 2, #6067, \n",
    "                    kegg_gene_brite = kegg_gene_brite) ) ),\n",
    "                    replace_chars = {'.':'_'})\n",
    "    # allowed boilerplate.\n",
    "\n",
    "    # initialize helper for input nodes\n",
    "    myvnn = VNNHelper(edge_dict = kegg_connections)\n",
    "\n",
    "    # Get a mapping of brite names to tensor list index\n",
    "    find_names = myvnn.nodes_inp # e.g. ['100383860', '100278565', ... ]\n",
    "    lookup_dict = {}\n",
    "\n",
    "    # the only difference lookup_dict and brite_node_to_list_idx_dict above is that this is made using the full set of genes in the list \n",
    "    # whereas that is made using kegg_gene_brite which is a subset\n",
    "    for i in range(len(parsed_kegg_gene_entries)):\n",
    "        if 'BRITE' not in parsed_kegg_gene_entries[i].keys():\n",
    "            pass\n",
    "        elif parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'] == []:\n",
    "            pass\n",
    "        else:\n",
    "            name = parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'][0][-1]\n",
    "            if name in find_names:\n",
    "                lookup_dict[name] = i\n",
    "    lookup_dict    \n",
    "\n",
    "    brite_node_to_list_idx_dict = {}\n",
    "    for i in tqdm(range(len(kegg_gene_brite))):\n",
    "        brite_node_to_list_idx_dict[str(kegg_gene_brite[i]['BRITE']['BRITE_PATHS'][0][-1])] = i        \n",
    "\n",
    "    # Get the input sizes for the graph\n",
    "    size_in_zip = zip(myvnn.nodes_inp, [np.prod(ACGT_gene_slice_list[lookup_dict[e]].shape[1:]) for e  in myvnn.nodes_inp])\n",
    "\n",
    "\n",
    "    # init input node sizes\n",
    "    myvnn.set_node_props(key = 'inp', node_val_zip = size_in_zip)\n",
    "\n",
    "    # # init node output sizes\n",
    "    myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_inp, [10 for e in myvnn.nodes_inp]))\n",
    "    myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_edge,[5 for e in myvnn.nodes_edge]))\n",
    "    myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_out, [30 for e in myvnn.nodes_out]))\n",
    "\n",
    "\n",
    "    # # options should be controlled by node_props\n",
    "    myvnn.set_node_props(key = 'flatten', node_val_zip = zip(\n",
    "        myvnn.nodes_inp, \n",
    "        [True for e in myvnn.nodes_inp]))\n",
    "\n",
    "    myvnn.set_node_props(key = 'reps', node_val_zip = zip(\n",
    "        myvnn.nodes_out+myvnn.nodes_inp+myvnn.nodes_edge, \n",
    "        [1 for e in myvnn.nodes_out+myvnn.nodes_inp+myvnn.nodes_edge]))\n",
    "\n",
    "    # init dropout \n",
    "    myvnn.set_node_props(key = 'drop', node_val_zip = zip(\n",
    "        myvnn.nodes_out+myvnn.nodes_inp+myvnn.nodes_edge, \n",
    "        [0.0 for e in myvnn.nodes_out+myvnn.nodes_inp+myvnn.nodes_edge]))\n",
    "\n",
    "    # init edge node input size (propagate forward input/edge outpus)\n",
    "    myvnn.calc_edge_inp()\n",
    "\n",
    "# myvnn.mk_digraph(include = ['node_name', 'inp_size', 'out_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c8ff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from above\n",
    "if demo_VNN_VAE: # Setup VNNHelper for Decoder\n",
    "    # reverse the edges in the edge dict (and deduplicate one to many relationships)\n",
    "    kegg_connections_reversed = reverse_edge_dict(edge_dict = kegg_connections)\n",
    "\n",
    "    # use existing graph to define properties of new one (switch inputs/outputs)\n",
    "    prop_dict_reversed = reverse_node_props(\n",
    "        prop_dict = myvnn.node_props, \n",
    "        conversion_dict = {'out':'inp',\n",
    "                           'inp':'out',\n",
    "                       'flatten':''})\n",
    "\n",
    "    # use the VNNHelper class to setup the connections but then pass in all the nodes' properties directly\n",
    "    myvnn_rev = VNNHelper(edge_dict = kegg_connections_reversed)\n",
    "    myvnn_rev.node_props = prop_dict_reversed\n",
    "    myvnn_rev.calc_edge_inp()\n",
    "# myvnn_rev.mk_digraph(include = ['node_name', 'inp_size', 'out_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa25408",
   "metadata": {},
   "outputs": [],
   "source": [
    "if demo_VNN_VAE: # Encoder, Decoder\n",
    "    model_vnn = VisableNeuralNetwork(\n",
    "        node_props = myvnn.node_props,\n",
    "        Linear_block = Linear_block_reps,\n",
    "        edge_dict = myvnn.edge_dict,\n",
    "        dependancy_order = myvnn.dependancy_order,\n",
    "        node_to_inp_num_dict = lookup_dict,\n",
    "    )\n",
    "\n",
    "    # now the reversed version\n",
    "    model_vnn_reverse = VisableNeuralNetwork(\n",
    "        node_props = myvnn_rev.node_props,\n",
    "        Linear_block = Linear_block_reps,\n",
    "        edge_dict = myvnn_rev.edge_dict,\n",
    "        dependancy_order = myvnn_rev.dependancy_order,\n",
    "        node_to_inp_num_dict = {'y_hat': 0},\n",
    "        return_dict = True # With multiple outputs a dictionary will automatically be returned.\n",
    "    )\n",
    "\n",
    "    # model_vnn_reverse([\n",
    "    #     model_vnn(next(iter(training_dataloader))[1]) \n",
    "    #     ])\n",
    "\n",
    "    VVH = VNNVAEHelper(\n",
    "        encoder = model_vnn, \n",
    "        decoder = model_vnn_reverse, \n",
    "        latent_inp_size = 30, \n",
    "        latent_size = 2)\n",
    "\n",
    "    # VVH(next(iter(training_dataloader))[1])\n",
    "    \n",
    "    plVVH = plVNNVAE(VVH, lookup_dict = lookup_dict)\n",
    "\n",
    "    logger = pl.loggers.TensorBoardLogger(\"tb_vnn_logs\", name=\"vnn-02.31-TESTING-REMOVE-ME\")\n",
    "    trainer = pl.Trainer(max_epochs=2, logger=logger)\n",
    "\n",
    "    trainer.fit(model=plVVH, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5b3ef5",
   "metadata": {},
   "source": [
    "## Training (general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b1e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, silent = False):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (xs_i, y_i) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(xs_i)\n",
    "        loss = loss_fn(pred, y_i) # <----------------------------------------\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(y_i) # <----------------\n",
    "            if not silent:\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22314751",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def train_error(dataloader, model, loss_fn, silent = False):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xs_i, y_i in dataloader:\n",
    "            pred = model(xs_i)\n",
    "            train_loss += loss_fn(pred, y_i).item() # <----------------------\n",
    "            \n",
    "    train_loss /= num_batches\n",
    "    return(train_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51151878",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, silent = False):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xs_i, y_i in dataloader:\n",
    "            pred = model(xs_i)\n",
    "            test_loss += loss_fn(pred, y_i).item() # <-----------------------\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    if not silent:\n",
    "        print(f\"Test Error: Avg loss: {test_loss:>8f}\")\n",
    "    return(test_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a7603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def yhat_loop(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    y_true = np.array([])\n",
    "    y_pred = np.array([])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xs_i, y_i in dataloader:\n",
    "            yhat_i = model(xs_i)\n",
    "            y_pred = np.append(y_pred, np.array(yhat_i.cpu()))\n",
    "            y_true = np.append(y_true, np.array(y_i.cpu()))\n",
    "    \n",
    "    out = np.concatenate([y_true[:, None], y_pred[:, None]], axis = 1) \n",
    "    out = pd.DataFrame(out, columns = ['y_true', 'y_pred'])\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c71c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def train_nn(\n",
    "    cache_path,\n",
    "    training_dataloader,\n",
    "    testing_dataloader,\n",
    "    model,\n",
    "    learning_rate = 1e-3,\n",
    "    batch_size = 64,\n",
    "    epochs = 500,\n",
    "    model_prefix = 'model'\n",
    "):\n",
    "    # Initialize the loss function\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])\n",
    "    loss_df['TrainMSE'] = np.nan\n",
    "    loss_df['TestMSE']  = np.nan\n",
    "\n",
    "    for t in tqdm(range(epochs)):        \n",
    "        # print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(training_dataloader, model, loss_fn, optimizer, silent = True)\n",
    "\n",
    "        loss_df.loc[loss_df.index == t, 'TrainMSE'\n",
    "                   ] = train_error(training_dataloader, model, loss_fn, silent = True)\n",
    "        \n",
    "        loss_df.loc[loss_df.index == t, 'TestMSE'\n",
    "                   ] = test_loop(testing_dataloader, model, loss_fn, silent = True)\n",
    "        \n",
    "        if (t+1)%5 == 0: # Cache in case training is interupted. \n",
    "            # print(loss_df.loc[loss_df.index == t, ['TrainMSE', 'TestMSE']])\n",
    "            torch.save(model.state_dict(), \n",
    "                       cache_path+'/'+model_prefix+'_'+str(t)+'_'+str(epochs)+'.pt') # convention is to use .pt or .pth\n",
    "        \n",
    "    return([model, loss_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c13dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363c0a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4b170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def estimate_iterations(sec_per_it = 161):\n",
    "    hours = [1, 2, 4, 8, 12, 24]\n",
    "    res = pd.DataFrame(zip(hours, \n",
    "    [math.floor(\n",
    "        ((i)*(60*60))/sec_per_it\n",
    "    ) for i in hours]), columns = ['Hours', 'Iterations'])\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707a9168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e62927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ACGTDataset(Dataset): # for any G containing matix with many (phno) to one (geno)\n",
    "    def __init__(self, \n",
    "                 y, \n",
    "                 G, # not on gpu\n",
    "                 idx_original,\n",
    "                 idx_lookup,\n",
    "                 transform = None, target_transform = None,\n",
    "                 use_gpu_num = 0,\n",
    "                 device = 'cuda',\n",
    "                 **kwargs \n",
    "                ):\n",
    "\n",
    "        self.device = device\n",
    "        self.y = y \n",
    "        self.G = G\n",
    "        self.idx_original = idx_original\n",
    "        self.idx_lookup = idx_lookup\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        y_idx =self.y[idx]\n",
    "            \n",
    "        #                 |array containing correct index in deduplicated g \n",
    "        #                 |               index in phno    \n",
    "        uniq_g_idx = self.idx_lookup[self.idx_original[idx], 1]\n",
    "        g_idx = self.G[uniq_g_idx, :, :]\n",
    "        \n",
    "        # send all to gpu        \n",
    "        if (self.device != 'cpu'):\n",
    "            if y_idx.device.type == 'cpu':\n",
    "                y_idx = y_idx.to(self.device) \n",
    "                \n",
    "            if g_idx.device.type == 'cpu':\n",
    "                g_idx = g_idx.to(self.device) \n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            g_idx = self.transform(g_idx)\n",
    "            \n",
    "        if self.target_transform:\n",
    "            y_idx = self.transform(y_idx)\n",
    "        return g_idx, y_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d74098",
   "metadata": {},
   "source": [
    "## For Multiple Inputs (but not requiring multiple inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6792dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "send_batch_to_gpu = 'cuda:0'\n",
    "# send_batch_to_gpu = '0'\n",
    "\n",
    "# res.to(int(ops.split(':')[-1]))\n",
    "\n",
    "if len(send_batch_to_gpu) >= 1:\n",
    "    # remove characters in 'cuda:'\n",
    "    send_batch_to_gpu = ''.join([e for e in send_batch_to_gpu if e not in ['c', 'u', 'd', 'a', ':']])\n",
    "\n",
    "send_batch_to_gpu = int(send_batch_to_gpu)\n",
    "send_batch_to_gpu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ccc3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BigDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lookup_obs,\n",
    "        lookups_are_filtered = False, # This is a critical piece of information. For deduplicated lookups to work they must either be filtered so that the \n",
    "                                      # lookup's length matchs y's length (idx will map to the right row) _or_ instead lookup_obs must contain a mapping \n",
    "                                      # from the y's current length to the row in the full dataset. \n",
    "                                      # If False idx -> lookup_obs[idx] -> lookup_env[idx] -> W[idx]\n",
    "                                      # If True  idx ------------------ -> lookup_env[idx] -> W[idx]\n",
    "#         lookup_geno,\n",
    "#         lookup_env,\n",
    "#         y,\n",
    "#         G, \n",
    "#         G_type\n",
    "#         S,\n",
    "#         P,\n",
    "#         W,\n",
    "#         W_type,\n",
    "#         send_batch_to_gpu # 'cuda:0' but '0' and 'cuda0' would also work\n",
    "        transform = None, \n",
    "        target_transform = None,\n",
    "        **kwargs \n",
    "        ):\n",
    "        \"\"\"\n",
    "        This class produces a set with one or more input tensors. For flexibility the only _required_ input is `lookup_obs`, a tensor with the index of observations. \n",
    "        Everything else is provided as a kwarg. Output is a list of tensors1 ordered [y, G, S, W], any of these not initalized will be missing but not empty (e.g. [y, S, W] not [y, None, S, W]).       \n",
    "        Used inputs are:\n",
    "        lookup_obs: index for y, used by __getitem__ for obs_idx\n",
    "        lookup_geno: index for G, row obs_idx, column 1 is geno_idx (geno information is deduplicated, hence the need for a lookup)\n",
    "        lookup_env: index for S & W, , row obs_idx, column 1 is env_idx (env information is deduplicated, hence the need for a lookup)\n",
    "        y: yield\n",
    "        G: Genomic information \n",
    "        G_type: how the infomation should be returned, 'raw', 'hilbert', or 'list' (i.e. of tensors for snps in each gene)\n",
    "        S: Soil information\n",
    "        P: Planting/Harvest date contained in column 0, 1 respectively \n",
    "        W: Weather data\n",
    "        W_type: how the infomation should be returned, 'raw' or 'hilbert'\n",
    "\n",
    "        1 G may also be returned as a list of tensors\n",
    "        \"\"\"\n",
    "        # Lookup info (so that deduplication works)\n",
    "        self.lookup_obs = lookup_obs\n",
    "        self.lookups_are_filtered = lookups_are_filtered\n",
    "        # if 'lookup_obs'  in kwargs: self.lookup_obs  = kwargs['lookup_obs'];\n",
    "        if 'lookup_geno' in kwargs: self.lookup_geno = kwargs['lookup_geno'];\n",
    "        if 'lookup_env'  in kwargs: self.lookup_env  = kwargs['lookup_env'];\n",
    "        # Data\n",
    "        if 'y' in kwargs: self.y = kwargs['y'];\n",
    "        if 'G' in kwargs: self.G = kwargs['G'];\n",
    "        if 'S' in kwargs: self.S = kwargs['S'];\n",
    "        if 'P' in kwargs: self.P = kwargs['P']; # PlantHarvest so that planting can be added into W\n",
    "        if 'W' in kwargs: self.W = kwargs['W'];\n",
    "        # Data prep state information\n",
    "        if 'G_type' in kwargs: self.G_type = kwargs['G_type']; # raw, hilbert, list\n",
    "        if 'W_type' in kwargs: self.W_type = kwargs['W_type']; # raw, hilbert\n",
    "        # Data to be returned\n",
    "        self.out_names = [e for e in ['y', 'G', 'S', 'W'] if e in kwargs]\n",
    "\n",
    "        # Is data starting on the desired device or does it need to be cycled on and off?\n",
    "        # This is based on ACGTDataset\n",
    "        self.send_batch_to_gpu = None\n",
    "        if 'send_batch_to_gpu' in kwargs.keys():\n",
    "            send_batch_to_gpu = kwargs['send_batch_to_gpu']\n",
    "            if type(send_batch_to_gpu) == str: \n",
    "                send_batch_to_gpu = send_batch_to_gpu.lower()\n",
    "                if len(send_batch_to_gpu) >= 1:\n",
    "                    # remove characters in 'cuda:'\n",
    "                    send_batch_to_gpu = ''.join([e for e in send_batch_to_gpu if e not in ['c', 'u', 'd', 'a', ':']])\n",
    "                self.send_batch_to_gpu = int(send_batch_to_gpu)\n",
    "            elif type(send_batch_to_gpu) == int: \n",
    "                self.send_batch_to_gpu = send_batch_to_gpu\n",
    "            else:\n",
    "                print('send_batch_to_gpu kwarg ignored. Must be a string or int. Ideally of form \"cuda:0\"')\n",
    "\n",
    "        # Transformations\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.lookup_obs)\n",
    "    \n",
    "\n",
    "    # These used to be in __getitem__ but separating them like this allows for them to be overwritten more easily\n",
    "    def get_y(self, idx):\n",
    "        y_idx = self.y[idx]\n",
    "        if self.transform:\n",
    "            y_idx = self.transform(y_idx)\n",
    "        return(y_idx)\n",
    "        \n",
    "    def get_G(self, idx):\n",
    "        geno_idx = self.lookup_geno[idx, 1]\n",
    "        if self.G_type in ['raw', 'hilbert']:\n",
    "            G_idx = self.G[geno_idx]\n",
    "        if 'list' == self.G_type:\n",
    "            G_idx = [e[geno_idx] for e in self.G]\n",
    "        if self.transform:\n",
    "            G_idx = self.transform(G_idx)\n",
    "        return(G_idx)\n",
    "\n",
    "    def get_S(self, idx):\n",
    "        env_idx = self.lookup_env[idx, 1]\n",
    "        S_idx = self.S[env_idx]\n",
    "        if self.transform:\n",
    "            S_idx = self.transform(S_idx)\n",
    "        return(S_idx)\n",
    "\n",
    "    def get_W(self, idx):\n",
    "        W_device = torch.Tensor(self.W).get_device()\n",
    "\n",
    "        env_idx = self.lookup_env[idx, 1]\n",
    "        # get growing information\n",
    "        WPlant = np.zeros(365)\n",
    "        # WPlant[self.P[obs_idx, 0]:self.P[obs_idx, 1]] = 1\n",
    "        WPlant[self.P[idx, 0]:self.P[idx, 1]] = 1\n",
    "        if self.W_type == 'raw':\n",
    "            WPlant = torch.from_numpy(WPlant).to(torch.float)\n",
    "            # if needed send to gpu\n",
    "            if W_device != -1: WPlant = WPlant.to(W_device)            \n",
    "            W_idx = torch.concatenate([self.W[env_idx], WPlant[None, :]], axis = 0)\n",
    "        if self.W_type == 'hilbert':\n",
    "            # convert growing info to hilbert curve\n",
    "            WPlant_hilb = np_3d_to_hilbert(WPlant[None, :, None], silent = True)\n",
    "            WPlant_hilb = WPlant_hilb.squeeze(axis = 3)\n",
    "            WPlant_hilb[np.isnan(WPlant_hilb)] = 0\n",
    "            WPlant_hilb = torch.from_numpy(WPlant_hilb).to(torch.float)\n",
    "            # if needed send to gpu\n",
    "            if W_device != -1: WPlant_hilb = WPlant_hilb.to(W_device)\n",
    "            W_idx = torch.concatenate([self.W[env_idx], WPlant_hilb], axis = 0)\n",
    "        if self.transform:\n",
    "            W_idx = self.transform(W_idx)\n",
    "        return(W_idx)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        out = []\n",
    "        obs_idx = idx\n",
    "        if self.lookups_are_filtered == False:\n",
    "            obs_idx = self.lookup_obs[idx]\n",
    "\n",
    "        if 'y' in self.out_names: out += [self.get_y(obs_idx)]\n",
    "        if 'G' in self.out_names: out += [self.get_G(obs_idx)]\n",
    "        if 'S' in self.out_names: out += [self.get_S(obs_idx)]\n",
    "        if 'W' in self.out_names: out += [self.get_W(obs_idx)]\n",
    "\n",
    "        # send all to gpu    \n",
    "        if self.send_batch_to_gpu is not None:\n",
    "            out = [[ee.to(self.send_batch_to_gpu) for ee in e] if type(e)==list else e.to(self.send_batch_to_gpu) for e in out]     \n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# using send_batch_to_gpu to reduce needed memory\n",
    "\n",
    "# X = g2fc_datawrapper()\n",
    "# X.set_split()\n",
    "# X.load_all(name_list = ['obs_geno_lookup', 'YMat', 'ACGT',], store=True) \n",
    "\n",
    "# X.calc_cs('YMat', version = 'np', filter = 'val:train')\n",
    "# X.calc_cs('ACGT',                 filter = 'val:train', filter_lookup= 'obs_geno_lookup')\n",
    "\n",
    "# training_dataloader = DataLoader(BigDataset(\n",
    "#     lookups_are_filtered = False,\n",
    "#     lookup_obs  = X.get('val:train',       ops_string='asarray from_numpy      '),\n",
    "#     lookup_geno = X.get('obs_geno_lookup', ops_string='asarray from_numpy      '),\n",
    "#     # y =           X.get('YMat',            ops_string='asarray from_numpy float')[:, None],\n",
    "#     # G =           X.get('ACGT',            ops_string='        from_numpy float'),\n",
    "#     # G_type = 'raw',\n",
    "#     G =           X.get('KEGG_slices',     ops_string='        from_numpy float'),\n",
    "#     G_type = 'list',\n",
    "#     send_batch_to_gpu = 'cuda:0'\n",
    "#     ),\n",
    "#     batch_size = 10,\n",
    "#     shuffle = True\n",
    "# )\n",
    "\n",
    "#                                      # 21184MiB needed for full dataset.\n",
    "# gi = next(iter(training_dataloader)) #   470MiB GPU used after being called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95623a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset is flexible but one must be cautios in setting it up.\n",
    "# Since weather and genomic data have duplicates the _deduplicated_ data is stored.\n",
    "# As a result there must be a mapping between yield and the covairates to be retrieved\n",
    "# This information is stored in lookup tables (lookup_env, lookup_geno)\n",
    "# This can be achieved in two ways: \n",
    "# 1. Take idx for y (which is filtered to only train/validation data), \n",
    "#    Lookup y's original idx using lookup_obs\n",
    "#    Use _that_ index to get the right enviromental/genomic index\n",
    "#    This approach is used if lookups_are_filtered == False\n",
    "# 2. Filter the lookup tables so that idx for y is the correct row in the lookup table\n",
    "#    This should be slightly less memory intensive (lookup tables are smaller) but is \n",
    "#    counting on the user to not forget to pre-filter these tables.\n",
    "#    this approach is used if lookups_are_filtered == True\n",
    "\n",
    "\n",
    "\n",
    "# if lookup_obs doesn't do anything then this filtering trick works.\n",
    "# note that here instead of filtering y to only the relevant obs i.e.:\n",
    "#     y =          X.get('YMat',           ops_string='filter:val:train asarray from_numpy float cuda:0'),\n",
    "# the full set is included.\n",
    "\n",
    "# training_dataloader = DataLoader(BigDataset(\n",
    "#     lookups_are_filtered = True,\n",
    "#     lookup_obs = X.get('val:train',      ops_string='                 asarray from_numpy             '),\n",
    "#     lookup_env = X.get('obs_env_lookup', ops_string='filter:val:train asarray from_numpy             '),\n",
    "#     y =          X.get('YMat',           ops_string='                 asarray from_numpy float cuda:0'),\n",
    "#     W =          X.get('WMat_hilb',      ops_string='                         from_numpy float cuda:0'),\n",
    "#     P =          X.get('PlantHarvest',   ops_string='                         from_numpy             '),\n",
    "#     W_type = 'hilbert'\n",
    "#     ),\n",
    "#     batch_size = 2,\n",
    "#     shuffle = True\n",
    "# )\n",
    "\n",
    "# training_dataloader = DataLoader(BigDataset(\n",
    "#     lookups_are_filtered = False,\n",
    "#     lookup_obs = X.get('val:train',      ops_string='asarray from_numpy             '),\n",
    "#     lookup_env = X.get('obs_env_lookup', ops_string='asarray from_numpy             '),\n",
    "#     y =          X.get('YMat',           ops_string='asarray from_numpy float cuda:0'),\n",
    "#     W =          X.get('WMat_hilb',      ops_string='        from_numpy float cuda:0'),\n",
    "#     P =          X.get('PlantHarvest',   ops_string='        from_numpy             '),\n",
    "#     W_type = 'hilbert'\n",
    "#     ),\n",
    "#     batch_size = 50,\n",
    "#     shuffle = True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2668965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Standard data prep\n",
    "\n",
    "# Wrapper function to hide the steps of loading data\n",
    "import numpy as np\n",
    "from EnvDL.core import get_cached_result\n",
    "from EnvDL.dlfn import read_split_info, find_idxs_split_dict\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "class g2fc_datawrapper():   \n",
    "    def __init__(self):\n",
    "        self.data_dict = {}\n",
    "        self.cs_dict = {}\n",
    "        print('Loading and storing default `phno`.')\n",
    "        self.load(name='phno', store = True)\n",
    "    \n",
    "\n",
    "    def set_split(self, load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/', json_prefix = '2023:9:5:12:8:26'):\n",
    "        if 'phno' not in self.data_dict.keys():\n",
    "            print('`phno` must be stored!\\nManually initialize with .load()')\n",
    "        else:\n",
    "            split_info = read_split_info(load_from = load_from, json_prefix = json_prefix)\n",
    "\n",
    "            temp = self.data_dict['phno'].copy()\n",
    "            temp[['Female', 'Male']] = temp['Hybrid'].str.split('/', expand = True)\n",
    "\n",
    "            self.test_dict = find_idxs_split_dict(\n",
    "                obs_df = temp, \n",
    "                split_dict = split_info['test'][0]\n",
    "            )\n",
    "\n",
    "            temp = temp.loc[self.test_dict['train_idx'], ] # restrict before re-aplying\n",
    "\n",
    "            self.val_dict = find_idxs_split_dict(\n",
    "                obs_df = temp, \n",
    "                split_dict = split_info['validate'][0]\n",
    "            )\n",
    "\n",
    "    def generic_load(self, load_from, file_name):        \n",
    "        if   file_name.split('.')[-1] == 'pkl': res = get_cached_result(load_from+file_name)\n",
    "        elif file_name.split('.')[-1] == 'npy': res = np.load(load_from+file_name)\n",
    "        elif file_name.split('.')[-1] == 'csv': res = pd.read_csv(load_from+file_name)\n",
    "        else: print(f'Unrecognized file encoding: {file_name.split(\".\")[-1]} \\nReturning None'); res = None\n",
    "        return res\n",
    "\n",
    "    def load(self, name='ACGT', store = False, **kwargs):\n",
    "        # defaults for quick access\n",
    "        defaults_dict = {\n",
    "            ## Genomic Data\n",
    "            'ACGT':         ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'ACGT.npy'],\n",
    "            'ACGT_hilb':    ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'ACGT_hilb.npy'],            \n",
    "            'KEGG_entries': ['../nbs_artifacts/01.05_g2fc_demo_model/', 'filtered_kegg_gene_entries.pkl'],\n",
    "            'KEGG_slices':  ['../nbs_artifacts/01.05_g2fc_demo_model/', 'ACGT_gene_slice_list.pkl'],\n",
    "\n",
    "            ## Soil and Management \n",
    "            'mgmtMatNames': ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'mgmtMatNames.npy'],\n",
    "            'mgmtMat':      ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'mgmtMat.npy'],\n",
    "            'SMatNames':    ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'SMatNames.npy'],\n",
    "            'SMat':         ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'SMat.npy'],\n",
    "\n",
    "            ## Weather\n",
    "            'PlantHarvestNames': ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'PlantHarvestNames.npy'],\n",
    "            'PlantHarvest':      ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'PlantHarvest.npy'],\n",
    "            'WMat':              ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'WMat.npy'],\n",
    "            'WMatNames':         ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'WMatNames.npy'],\n",
    "            'WMat_hilb':         ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'WMat_hilb.npy'],\n",
    "\n",
    "            # Response and lookup\n",
    "            'phno':            ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'phno_geno.csv'],\n",
    "            'obs_geno_lookup': ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'obs_geno_lookup.npy'], # Phno_Idx  Geno_Idx  Is_Phno_Idx\n",
    "            'obs_env_lookup':  ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'obs_env_lookup.npy'],  # Phno_Idx  Env_Idx   Is_Phno_Idx\n",
    "            'YMat':            ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'YMat.npy']\n",
    "        }\n",
    "\n",
    "        if name in defaults_dict.keys():\n",
    "            load_from, file_name = defaults_dict[name]\n",
    "        else: \n",
    "            print(f'`name` not recognized. Use `load_from` and `file_name` for greater control.\\,Allowed `names` are:\\n{list(defaults_dict.keys())}')\n",
    "        \n",
    "        # overwrite defaults if desired\n",
    "        if 'load_from' in kwargs.keys(): load_from = kwargs['load_from']\n",
    "        if 'file_name' in kwargs.keys(): file_name = kwargs['file_name']\n",
    "\n",
    "        res = self.generic_load(load_from=load_from, file_name= file_name)\n",
    "\n",
    "        if store:\n",
    "            self.data_dict[name] = res\n",
    "        else:\n",
    "            return res\n",
    "\n",
    "    def load_all(self, name_list = [], store = False):\n",
    "        if store:\n",
    "            for e in name_list:\n",
    "                self.load(name = e, store=store)\n",
    "        else:\n",
    "            res_list = []\n",
    "            for e in name_list:\n",
    "                res_list += [self.load(name = e, store=store)]\n",
    "            return res_list\n",
    "\n",
    "    def store_cs(self, name, cs_list):\n",
    "        self.cs_dict[name] = cs_list\n",
    "\n",
    "    def calc_cs(self, name, version = 'np', **kwargs):\n",
    "\n",
    "        res = self.data_dict[name]\n",
    "        if 'filter' in kwargs.keys():\n",
    "            which_dict, which_split = kwargs['filter'].split(':')\n",
    "\n",
    "            if which_split == 'train':  key = 'train_idx'\n",
    "            elif which_split == 'test': key = 'test_idx'\n",
    "            else: print('only `train` and `test` indexes are allowed.')\n",
    "\n",
    "            if which_dict == 'val':    mask = self.val_dict[key]\n",
    "            elif which_dict == 'test': mask = self.test_dict[key]\n",
    "            else: print('only `val` and `test` sets are allowed.')\n",
    "\n",
    "            if 'filter_lookup' in kwargs.keys():\n",
    "                # This block exists because some data is deduplicated. In the dataloader I use lookup tables to find the right values.\n",
    "                # That gets messy because the enviroment, genome, and yield all get different ones\n",
    "                # I could hardcode names to filters but that would make this code pretty inflexible (which I would like to avoid.)\n",
    "                # using the manual overwrite metod .store_cs() it's possible to get the desired behavior like this:\n",
    "                # X.store_cs('WMat', calc_cs(X.get('WMat')[np.array(list(set(X.get('obs_env_lookup', ops_string='filter:val:train')[:, 1]))),: ,:]))\n",
    "                # That is a lot messier looking than I would like. It's hard to see what's happening. \n",
    "                # To get around this I'm adding a 'filter_lookup' kwarg that does the same job as the lookup tables in the data loader.\n",
    "                lookup = self.data_dict[kwargs['filter_lookup']]\n",
    "                lookup = lookup[mask, 1]\n",
    "                # deduplicate; for cs we don't need the order of the obs.\n",
    "                mask = np.array(list(set(lookup)))\n",
    "            res = res[mask]\n",
    "\n",
    "        else:\n",
    "            print('''\n",
    "Scaling based on ALL data. To avoid this pass in a split to be used. \n",
    "If a lookup table should be used to select observations (e.g. obs_env_lookup ) its name should be passed in. \n",
    "E.g. filter = \\'val:train\\',  filter_lookup = \\'obs_env_lookup\\'\n",
    "                  ''')\n",
    "\n",
    "        if version == 'np':\n",
    "            self.cs_dict[name] = [np.asarray(np.mean(res, axis = 0)), np.asarray(np.std(res, axis = 0))]\n",
    "        elif version == 'torch':\n",
    "            self.cs_dict[name] = [torch.Tensor.mean(res, axis = 0), torch.Tensor.std(res, axis = 0)]\n",
    "\n",
    "    def calc_cs_all(self, name_list, version = 'np', **kwargs):\n",
    "        for name in name_list:\n",
    "            self.calc_cs(name=name, version = version, **kwargs)\n",
    "\n",
    "    def apply_cs(self, name, **kwargs):\n",
    "        if name not in self.cs_dict.keys():\n",
    "            self.calc_cs(name, kwargs)\n",
    "\n",
    "        vals = self.cs_dict[name]\n",
    "        res = self.data_dict[name]\n",
    "        \n",
    "        if type(res) == type(vals[0]):\n",
    "            pass\n",
    "        elif type(res) == torch.Tensor:\n",
    "            # convert to pytorch\n",
    "            vals = [torch.from_numpy(e) for e in vals]\n",
    "        elif type(res) == np.ndarray:\n",
    "            # convert to numpy\n",
    "            vals = [torch.Tensor.numpy(e) for e in vals]\n",
    "            \n",
    "        center, scale = vals\n",
    "        res = (res - center) / scale\n",
    "        return res\n",
    "\n",
    "    def get_cs(self, name):\n",
    "        if name not in self.cs_dict.keys():\n",
    "            res = None\n",
    "        else:\n",
    "            res = self.cs_dict[name]\n",
    "        return res\n",
    "\n",
    "    def reverse_cs(self, name, x):\n",
    "        vals = self.cs_dict[name]\n",
    "\n",
    "        if type(x) == type(vals[0]):\n",
    "            pass\n",
    "        elif type(x) == torch.Tensor:\n",
    "            # convert to pytorch\n",
    "            vals = [torch.from_numpy(e) for e in vals]\n",
    "        elif type(res) == np.ndarray:\n",
    "            # convert to numpy\n",
    "            vals = [torch.Tensor.numpy(e) for e in vals]\n",
    "\n",
    "        center, scale = vals\n",
    "        res = (res * scale) + center\n",
    "        return res\n",
    "\n",
    "    \n",
    "    def get(self, name, ops_string = ''):\n",
    "        # is split info being requested? (for lookup_obs most likely). Otherwise main data is being requested.\n",
    "        if name not in ['val:train', 'test:train', 'val:test', 'test:test']:\n",
    "            if name not in self.data_dict.keys():\n",
    "                self.load(name, store=True)\n",
    "            res = self.data_dict[name]\n",
    "        else: \n",
    "            which_dict, which_split = name.split(':')\n",
    "\n",
    "            if which_split == 'train':  key = 'train_idx'\n",
    "            elif which_split == 'test': key = 'test_idx'\n",
    "            else: print('only `train` and `test` indexes are allowed.')\n",
    "\n",
    "            if which_dict == 'val':    res = self.val_dict[key]\n",
    "            elif which_dict == 'test': res = self.test_dict[key]\n",
    "            else: print('only `val` and `test` sets are allowed.')\n",
    "            \n",
    "        # apply opperations\n",
    "        ops_string = [e for e in ops_string.split(' ') if e != '']\n",
    "        for ops in ops_string:\n",
    "            if ops[0:6] == 'filter':\n",
    "                #FIXME Identified a bug on 12/14 where the ops string \n",
    "                # 'cs filter:val:train asarray from_numpy float cuda:0\n",
    "                # works exactly as expected but filtering before cs\n",
    "                # 'filter:val:train cs asarray from_numpy float cuda:0\n",
    "                # causes NO filtering to occur. \n",
    "                _, which_dict, which_split = ops.split(':')\n",
    "\n",
    "                if which_split == 'train':  key = 'train_idx'\n",
    "                elif which_split == 'test': key = 'test_idx'\n",
    "                else: print('only `train` and `test` indexes are allowed.')\n",
    "\n",
    "                if which_dict == 'val':    res_idx = self.val_dict[key]\n",
    "                elif which_dict == 'test': res_idx = self.test_dict[key]\n",
    "                else: print('only `val` and `test` sets are allowed.')\n",
    "\n",
    "                if type(res) == list: \n",
    "                    res = [e[res_idx] for e in res]\n",
    "                else:\n",
    "                    res = res[res_idx]\n",
    "\n",
    "            if ops == 'cs':\n",
    "                res = self.apply_cs(name)\n",
    "            \n",
    "            if ops == 'asarray':\n",
    "                if type(res) == list: \n",
    "                    res = [np.asarray(e) for e in res]\n",
    "                else:\n",
    "                    res = np.asarray(res)\n",
    "            if ops == 'from_numpy':\n",
    "                if type(res) == list: \n",
    "                    res = [torch.from_numpy(e) for e in res]\n",
    "                else:\n",
    "                    res = torch.from_numpy(res)\n",
    "            if ops == 'float':\n",
    "                if type(res) == list: \n",
    "                    res = [e.to(torch.float) for e in res]\n",
    "                else:\n",
    "                    res = res.to(torch.float)\n",
    "            if ops[0:4] == 'cuda':\n",
    "                # send to device by number. e.g. cuda:0 -> X.to(0)\n",
    "                if type(res) == list: \n",
    "                    dev_int = int(ops.split(':')[-1])\n",
    "                    res = [e.to(dev_int) for e in res]\n",
    "                else:\n",
    "                    res = res.to(int(ops.split(':')[-1]))\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "# some example usage \n",
    "# X = g2fc_datawrapper()\n",
    "# X.set_split()\n",
    "# X.load_all(name_list = ['obs_env_lookup', 'YMat', 'PlantHarvest', 'WMat',], store=True) \n",
    "# X.calc_cs('YMat', version = 'np', filter = 'val:train'); X.cs_dict['YMat']\n",
    "# X.calc_cs_all(['YMat'], version = 'np', filter = 'val:train'); X.cs_dict['YMat']\n",
    "# X.calc_cs('YMat', version = 'np'); X.cs_dict['YMat']\n",
    "\n",
    "# some demonstration of when to use kwargs for scaling \n",
    "# how do I manually do scaling for enviromental things?\n",
    "# X.store_cs('WMat', calc_cs(X.get('WMat')[np.array(list(set(X.get('obs_env_lookup', ops_string='filter:val:train')[:, 1]))),: ,:]))\n",
    "# [e[0:3, 0] for e in X.cs_dict['WMat']]\n",
    "# X.calc_cs('WMat', filter = 'val:train', filter_lookup= 'obs_env_lookup')\n",
    "# [e[0:3, 0] for e in X.cs_dict['WMat']]\n",
    "# X.calc_cs('WMat')\n",
    "# [e[0:3, 0] for e in X.cs_dict['WMat']]\n",
    "\n",
    "# X.get('WMat', ops_string='asarray')[0:3, 0:3, 0]\n",
    "# X.get('WMat', ops_string='cs asarray')[0:3, 0:3, 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b0972b",
   "metadata": {},
   "source": [
    "## PyTorch Lighning Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9adb6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class plDNN_general(pl.LightningModule):\n",
    "    def __init__(self, mod, log_weight_stats = False):\n",
    "        super().__init__()\n",
    "        self.mod = mod\n",
    "        self.log_weight_stats = log_weight_stats\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_i, x_i = batch\n",
    "        pred = self.mod(x_i)\n",
    "        loss = F.mse_loss(pred, y_i)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        if self.log_weight_stats:\n",
    "            with torch.no_grad():\n",
    "                weight_list=[(name, param) for name, param in model.named_parameters() if name.split('.')[-1] == 'weight']\n",
    "                for l in weight_list:\n",
    "                    self.log((\"train_mean\"+l[0]), l[1].mean())\n",
    "                    self.log((\"train_std\"+l[0]), l[1].std())        \n",
    "\n",
    "        return(loss)\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_i, x_i = batch\n",
    "        pred = self.mod(x_i)\n",
    "        loss = F.mse_loss(pred, y_i)\n",
    "        self.log('val_loss', loss)        \n",
    "     \n",
    "    def configure_optimizers(self, **kwargs):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), **kwargs)\n",
    "        return optimizer    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73648362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2155ccdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a980bdc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdce66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444bd667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c83618e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562d15a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0f986d6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a5f1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26867472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3723f05e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c9b06b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08098c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abbfd02f",
   "metadata": {},
   "source": [
    "## Functions for Visible Neural Nets (y first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404764b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def train_loop_yx(dataloader, model, loss_fn, optimizer, silent = False):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (y_i, xs_i) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(xs_i)\n",
    "        \n",
    "        # ensure both are on cuda\n",
    "        if pred.device.type == 'cpu':\n",
    "            pred = pred.to('cuda')\n",
    "        if y_i.device.type == 'cpu':\n",
    "            y_i = y_i.to('cuda')\n",
    "        \n",
    "        loss = loss_fn(pred, y_i)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(y_i) \n",
    "            if not silent:\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebfd2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def train_error_yx(dataloader, model, loss_fn, silent = False):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for y_i, xs_i in dataloader:\n",
    "            pred = model(xs_i)\n",
    "            \n",
    "            # ensure both are on cuda\n",
    "            if pred.device.type == 'cpu':\n",
    "                pred = pred.to('cuda')\n",
    "            if y_i.device.type == 'cpu':\n",
    "                y_i = y_i.to('cuda')\n",
    "            \n",
    "            train_loss += loss_fn(pred, y_i).item()\n",
    "            \n",
    "    train_loss /= num_batches\n",
    "    return(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def test_loop_yx(dataloader, model, loss_fn, silent = False):   \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for y_i, xs_i in dataloader:\n",
    "            pred = model(xs_i)\n",
    "            \n",
    "            # ensure both are on cuda\n",
    "            if pred.device.type == 'cpu':\n",
    "                pred = pred.to('cuda')\n",
    "            if y_i.device.type == 'cpu':\n",
    "                y_i = y_i.to('cuda')\n",
    "                \n",
    "            test_loss += loss_fn(pred, y_i).item() \n",
    "\n",
    "    test_loss /= num_batches\n",
    "    if not silent:\n",
    "        print(f\"Test Error: Avg loss: {test_loss:>8f}\")\n",
    "    return(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79c78d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def train_nn_yx(\n",
    "    cache_path,\n",
    "    training_dataloader,\n",
    "    testing_dataloader,\n",
    "    model,\n",
    "    batch_size = 64,\n",
    "    epochs = 500,\n",
    "    model_prefix = 'model',\n",
    "    save_model = False,\n",
    "    **kwargs # can include 'silent' for train loop or 'save_on' for saving frequency\n",
    "):   \n",
    "    if 'optimizer' not in kwargs:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=kwargs['learning_rate'])\n",
    "    else:\n",
    "        optimizer = kwargs['optimizer']\n",
    "        \n",
    "    if 'save_on' in kwargs:\n",
    "        save_on = kwargs['save_on']\n",
    "    else:\n",
    "        save_on = 5       \n",
    "    \n",
    "    # Initialize the loss function\n",
    "    loss_fn = nn.MSELoss()     \n",
    "\n",
    "    loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])\n",
    "    loss_df['TrainMSE'] = np.nan\n",
    "    loss_df['TestMSE']  = np.nan\n",
    "\n",
    "    for t in tqdm(range(epochs)):        \n",
    "        if 'silent' in kwargs:\n",
    "            train_loop_yx(training_dataloader, model, loss_fn, optimizer, silent = kwargs['silent'])\n",
    "        else:\n",
    "            train_loop_yx(training_dataloader, model, loss_fn, optimizer, silent = True)\n",
    "\n",
    "        loss_df.loc[loss_df.index == t, 'TrainMSE'\n",
    "                   ] = train_error_yx(training_dataloader, model, loss_fn, silent = True)\n",
    "        \n",
    "        loss_df.loc[loss_df.index == t, 'TestMSE'\n",
    "                   ] = test_loop_yx(testing_dataloader, model, loss_fn, silent = True)\n",
    "        \n",
    "        if (t+1)%save_on == 0: # Cache in case training is interupted. \n",
    "            if save_model:\n",
    "                torch.save(model.state_dict(), \n",
    "                           cache_path+'/'+model_prefix+'_'+str(t)+'_'+str(epochs)+'.pt') # convention is to use .pt or .pth\n",
    "        \n",
    "    return([model, loss_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccbe2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def yhat_loop_yx(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    y_true = np.array([])\n",
    "    y_pred = np.array([])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for y_i, xs_i in dataloader:\n",
    "            yhat_i = model(xs_i)\n",
    "            y_pred = np.append(y_pred, np.array(yhat_i.cpu()))\n",
    "            y_true = np.append(y_true, np.array(y_i.cpu()))\n",
    "    \n",
    "    out = np.concatenate([y_true[:, None], y_pred[:, None]], axis = 1) \n",
    "    out = pd.DataFrame(out, columns = ['y_true', 'y_pred'])\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b556ceeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711e096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00dceeb3",
   "metadata": {},
   "source": [
    "## Functions from multi-trait output tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeb862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "\n",
    "# def train_loop3(dataloader, model, loss_fn, optimizer, silent = False):\n",
    "#     \"This is a version of train_loop which concatenates three ys.\"\n",
    "#     import torch\n",
    "#     from torch.utils.data import Dataset\n",
    "#     from torch.utils.data import DataLoader\n",
    "#     size = len(dataloader.dataset)\n",
    "#     for batch, (xs_i, y1_i, y2_i, y3_i) in enumerate(dataloader):\n",
    "#         # Compute prediction and loss\n",
    "#         pred = model(xs_i)\n",
    "#         loss = loss_fn(pred, torch.concat([y1_i, y2_i, y3_i], axis = 1)) # <----------------------------------------\n",
    "\n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if batch % 100 == 0:\n",
    "#             loss, current = loss.item(), batch * len(y1_i) # <----------------\n",
    "#             if not silent:\n",
    "#                 print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef067bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0904feee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "\n",
    "# def train_error3(dataloader, model, loss_fn, silent = False):\n",
    "#     import torch\n",
    "#     from torch.utils.data import Dataset\n",
    "#     from torch.utils.data import DataLoader\n",
    "#     size = len(dataloader.dataset)\n",
    "#     num_batches = len(dataloader)\n",
    "#     train_loss = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for xs_i, y1_i, y2_i, y3_i in dataloader:\n",
    "#             pred = model(xs_i)\n",
    "#             train_loss += loss_fn(pred, torch.concat([y1_i, y2_i, y3_i], axis = 1)).item() # <----------------------\n",
    "            \n",
    "#     train_loss /= num_batches\n",
    "#     return(train_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9f0fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d9cf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "\n",
    "# def test_loop3(dataloader, model, loss_fn, silent = False):\n",
    "#     import torch\n",
    "#     from torch.utils.data import Dataset\n",
    "#     from torch.utils.data import DataLoader\n",
    "\n",
    "#     size = len(dataloader.dataset)\n",
    "#     num_batches = len(dataloader)\n",
    "#     test_loss = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for xs_i, y1_i, y2_i, y3_i in dataloader:\n",
    "#             pred = model(xs_i)\n",
    "#             test_loss += loss_fn(pred, torch.concat([y1_i, y2_i, y3_i], axis = 1)).item() # <-----------------------\n",
    "\n",
    "#     test_loss /= num_batches\n",
    "#     if not silent:\n",
    "#         print(f\"Test Error: Avg loss: {test_loss:>8f}\")\n",
    "#     return(test_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c78817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6745d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962ace40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "\n",
    "# def yhat_loop3(dataloader, model):\n",
    "#     \"Version of yhat_loop that returns 3 ys\"\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "#     import torch\n",
    "#     size = len(dataloader.dataset)\n",
    "#     num_batches = len(dataloader)\n",
    "    \n",
    "#     first_loop = True\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for xs_i, y1_i, y2_i, y3_i in dataloader:\n",
    "#             yhat_i = model(xs_i)\n",
    "#             y_i = torch.concat([y1_i, y2_i, y3_i], axis = 1) # <-----------------------\n",
    "\n",
    "#             if first_loop:\n",
    "#                 y_pred = np.array(yhat_i.cpu())\n",
    "#                 y_true = np.array(y_i.cpu())\n",
    "#                 first_loop = False\n",
    "#             else:            \n",
    "#                 y_pred = np.concatenate([y_pred, np.array(yhat_i.cpu())])\n",
    "#                 y_true = np.concatenate([y_true, np.array(y_i.cpu())])\n",
    "                \n",
    "#     out = np.concatenate([y_true[:, :], y_pred[:, :]], axis = 1) \n",
    "#     out = pd.DataFrame(out, columns = ['y1_true', 'y2_true', 'y3_true', 'y1_pred', 'y2_pred', 'y3_pred'])\n",
    "#     return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b659c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
