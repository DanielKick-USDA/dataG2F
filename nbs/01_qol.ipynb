{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality of Life\n",
    "\n",
    "> Functions to improve quality of life. Duplicates a subset of those in `EnvDL`'s `core`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp qol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for working with files, directories with the intention of caching results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_txt(path, \n",
    "             **kwargs # Intended to allow for explicit 'encoding' to be passed into open the file\n",
    "            ):\n",
    "    if 'encoding' in kwargs.keys():\n",
    "        print(kwargs)\n",
    "        with open(path, 'r', encoding  = kwargs['encoding']) as f:\n",
    "            data = f.read()        \n",
    "    else:    \n",
    "        with open(path, 'r') as f:\n",
    "            data = f.read()\n",
    "            \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def print_txt(path):\n",
    "    print(read_txt(path = path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def read_json(json_path\n",
    "             ):\n",
    "    \"Read and return json. Used for train/validation/test splits\"\n",
    "    import json\n",
    "    with open(json_path, 'r') as fp:\n",
    "        dat = json.load(fp)\n",
    "    return(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def ensure_dir_path_exists(dir_path = '../ext_data' # Directory path to check\n",
    "                          ):\n",
    "    \"Iteratively check for and create directories to store output. Ideally this would just be os.mkdirs() but that function is not available in this version of python\"\n",
    "    import os\n",
    "    \n",
    "    for i in range(2, len(dir_path.split('/'))+1):\n",
    "        path_part = '/'.join(dir_path.split('/')[0:i])\n",
    "        if not os.path.exists(path_part):\n",
    "            os.mkdir(path_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\"Retrieve a previously calculated result. Return None if it cannot be found.\"\n",
    "def get_cached_result(\n",
    "    save_path\n",
    "):\n",
    "    import os\n",
    "    import pickle as pkl\n",
    "#     import pickle5 as pkl # Using non-base version of pickle \n",
    "#                           # conda env with gpu support for tf and torch uses python 3.7.\n",
    "#                           # Python 3.7 doesn't contain pickle v 5\n",
    "    if not os.path.exists(save_path):\n",
    "        cached_result = None\n",
    "    else:\n",
    "        with open(save_path, 'rb') as handle:\n",
    "                cached_result = pkl.load(handle)\n",
    "    return(cached_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def put_cached_result(\n",
    "    save_path,\n",
    "    save_obj\n",
    "):\n",
    "    import pickle as pkl\n",
    "#     import pickle5 as pkl\n",
    "#     from EnvDL.core import ensure_dir_path_exists\n",
    "    ensure_dir_path_exists(dir_path= '/'.join(save_path.split('/')[:-1]) )\n",
    "    \n",
    "    with open(save_path, 'wb') as handle:\n",
    "            pkl.dump(save_obj, \n",
    "                     handle, \n",
    "                     protocol=4 # version 4 is used instead of 5 because the container\n",
    "                                # I'm using with tf and torch uses python 3.7 and version\n",
    "                                # 5 is introduced in 3.8\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def remove_matching_files(\n",
    "    cache_path, # Directory to query\n",
    "    match_regex_list = ['.*\\.pt', 'yhats\\.csv', 'loss_df\\.csv'], # List of regexes to match (okay if two regexes match the same entry)\n",
    "    dry_run = True # Print files to be deleted or delete them. \n",
    "):\n",
    "    \"Helper function to clear out cache. Remove files from a folder if they match one of a given set of regexes. Ignores directories in directory. Useful for clearing out model artifacts.\"\n",
    "    import os\n",
    "    import re\n",
    "    # if empty set is provided, match nothing.\n",
    "    if match_regex_list == []:\n",
    "        match_regex_list = ['']\n",
    "    \n",
    "    files_to_remove = [[e for e in os.listdir(cache_path) if re.match(match_regex, e)\n",
    "                       ] for match_regex in match_regex_list]\n",
    "    # make a (potential) list of lists into a flat list\n",
    "    new_list = []\n",
    "    for sub_list in files_to_remove:\n",
    "        new_list = new_list + sub_list\n",
    "    # ensure it's deduplicated in case two regexes match with the same item\n",
    "    files_to_remove = list(set(new_list))\n",
    "    # remove any directories from consideration\n",
    "    files_to_remove = [e for e in files_to_remove if os.path.isfile(cache_path+e)]\n",
    "    # sort to make output more pleasant\n",
    "    files_to_remove.sort()\n",
    "\n",
    "    if files_to_remove == []:\n",
    "        print('No files found to remove.')\n",
    "    else:\n",
    "        if dry_run:\n",
    "            print('Command would remove:')\n",
    "            print('\\n'.join(files_to_remove))\n",
    "        else:\n",
    "            for file in files_to_remove:\n",
    "                os.remove(cache_path+file)\n",
    "\n",
    "# remove_matching_files(\n",
    "#     cache_path,\n",
    "#     match_regex_list = ['.*\\.pt', 'yhats\\.csv', 'loss_df\\.csv'],\n",
    "#     dry_run = False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
