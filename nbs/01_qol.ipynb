{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality of Life\n",
    "\n",
    "> Functions to improve quality of life. Duplicates a subset of those in `EnvDL`'s `core`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp qol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for working with files, directories with the intention of caching results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_txt(path, \n",
    "             **kwargs # Intended to allow for explicit 'encoding' to be passed into open the file\n",
    "            ):\n",
    "    if 'encoding' in kwargs.keys():\n",
    "        print(kwargs)\n",
    "        with open(path, 'r', encoding  = kwargs['encoding']) as f:\n",
    "            data = f.read()        \n",
    "    else:    \n",
    "        with open(path, 'r') as f:\n",
    "            data = f.read()\n",
    "            \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def print_txt(path):\n",
    "    print(read_txt(path = path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def read_json(json_path\n",
    "             ):\n",
    "    \"Read and return json. Used for train/validation/test splits\"\n",
    "    import json\n",
    "    with open(json_path, 'r') as fp:\n",
    "        dat = json.load(fp)\n",
    "    return(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def ensure_dir_path_exists(dir_path = '../ext_data' # Directory path to check\n",
    "                          ):\n",
    "    \"Iteratively check for and create directories to store output. Ideally this would just be os.mkdirs() but that function is not available in this version of python\"\n",
    "    import os\n",
    "    \n",
    "    for i in range(2, len(dir_path.split('/'))+1):\n",
    "        path_part = '/'.join(dir_path.split('/')[0:i])\n",
    "        if not os.path.exists(path_part):\n",
    "            os.mkdir(path_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\"Retrieve a previously calculated result. Return None if it cannot be found.\"\n",
    "def get_cached_result(\n",
    "    save_path\n",
    "):\n",
    "    import os\n",
    "    import pickle as pkl\n",
    "#     import pickle5 as pkl # Using non-base version of pickle \n",
    "#                           # conda env with gpu support for tf and torch uses python 3.7.\n",
    "#                           # Python 3.7 doesn't contain pickle v 5\n",
    "    if not os.path.exists(save_path):\n",
    "        cached_result = None\n",
    "    else:\n",
    "        with open(save_path, 'rb') as handle:\n",
    "                cached_result = pkl.load(handle)\n",
    "    return(cached_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def put_cached_result(\n",
    "    save_path,\n",
    "    save_obj\n",
    "):\n",
    "    import pickle as pkl\n",
    "#     import pickle5 as pkl\n",
    "#     from EnvDL.core import ensure_dir_path_exists\n",
    "    ensure_dir_path_exists(dir_path= '/'.join(save_path.split('/')[:-1]) )\n",
    "    \n",
    "    with open(save_path, 'wb') as handle:\n",
    "            pkl.dump(save_obj, \n",
    "                     handle, \n",
    "                     protocol=4 # version 4 is used instead of 5 because the container\n",
    "                                # I'm using with tf and torch uses python 3.7 and version\n",
    "                                # 5 is introduced in 3.8\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def remove_matching_files(\n",
    "    cache_path, # Directory to query\n",
    "    match_regex_list = ['.*\\.pt', 'yhats\\.csv', 'loss_df\\.csv'], # List of regexes to match (okay if two regexes match the same entry)\n",
    "    dry_run = True # Print files to be deleted or delete them. \n",
    "):\n",
    "    \"Helper function to clear out cache. Remove files from a folder if they match one of a given set of regexes. Ignores directories in directory. Useful for clearing out model artifacts.\"\n",
    "    import os\n",
    "    import re\n",
    "    # if empty set is provided, match nothing.\n",
    "    if match_regex_list == []:\n",
    "        match_regex_list = ['']\n",
    "    \n",
    "    files_to_remove = [[e for e in os.listdir(cache_path) if re.match(match_regex, e)\n",
    "                       ] for match_regex in match_regex_list]\n",
    "    # make a (potential) list of lists into a flat list\n",
    "    new_list = []\n",
    "    for sub_list in files_to_remove:\n",
    "        new_list = new_list + sub_list\n",
    "    # ensure it's deduplicated in case two regexes match with the same item\n",
    "    files_to_remove = list(set(new_list))\n",
    "    # remove any directories from consideration\n",
    "    files_to_remove = [e for e in files_to_remove if os.path.isfile(cache_path+e)]\n",
    "    # sort to make output more pleasant\n",
    "    files_to_remove.sort()\n",
    "\n",
    "    if files_to_remove == []:\n",
    "        print('No files found to remove.')\n",
    "    else:\n",
    "        if dry_run:\n",
    "            print('Command would remove:')\n",
    "            print('\\n'.join(files_to_remove))\n",
    "        else:\n",
    "            for file in files_to_remove:\n",
    "                os.remove(cache_path+file)\n",
    "\n",
    "# remove_matching_files(\n",
    "#     cache_path,\n",
    "#     match_regex_list = ['.*\\.pt', 'yhats\\.csv', 'loss_df\\.csv'],\n",
    "#     dry_run = False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for working with DataFrames, especially cleaning them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def find_df_shared_cols(df1,# DataFrame 1 \n",
    "                        df2 # DataFrame 2\n",
    "                       ):\n",
    "    shared_cols = [e for e in list(df1) if e in list(df2)]\n",
    "    return(shared_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame.from_dict({'a':[0], 'b':['c']})\n",
    "df2 = pd.DataFrame.from_dict({'a':[1], 'b':[0]})\n",
    "\n",
    "find_df_shared_cols(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def find_df_col_mismatches(df1, # DataFrame 1 \n",
    "                           df2, # DataFrame 2 \n",
    "                           showtype = True # Whether the data types should be returned\n",
    "                          ):\n",
    "    \"Identify columns that match in two dataframes but have mismatched data types\"\n",
    "    import numpy as np\n",
    "    if showtype:\n",
    "        return [(e, np.dtype(df1[e]), np.dtype(df2[e])) for e in [e for e in list(df2) if e in list(df1)] if (np.dtype(df1[e]) != np.dtype(df2[e])) ]\n",
    "    else:\n",
    "        return [e for e in [e for e in list(df2) if e in list(df1)] if (np.dtype(df1[e]) != np.dtype(df2[e])) ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', dtype('O'), dtype('int64'))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame.from_dict({'a':[0], 'b':['c']})\n",
    "df2 = pd.DataFrame.from_dict({'a':[1], 'b':[0]})\n",
    "\n",
    "find_df_col_mismatches(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def summarize_col_missing(df):\n",
    "    \"Report the number and percentage of missing values for a DataFrame\"\n",
    "    import pandas as pd\n",
    "    return(\n",
    "        pd.DataFrame({'Col'   : [e for e in list(df)],\n",
    "              'N_miss' : [sum(df[e].isna()) for e in list(df)],\n",
    "              'Pr_Comp': [round(100*(1-sum(df[e].isna())/len(df[e])), 1) for e in list(df)]})\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col</th>\n",
       "      <th>N_miss</th>\n",
       "      <th>Pr_Comp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>83.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Col  N_miss  Pr_Comp\n",
       "0   a       1     83.3\n",
       "1   b       0    100.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame.from_dict({'a':[0, 1, 2, 3, 4, np.nan], \n",
    "                              'b':[0, 1, 2, 3, 4, 5     ]})\n",
    "\n",
    "summarize_col_missing(df= df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def sanitize_col(df, # DataFrame\n",
    "                 col, # Column to be sanitized\n",
    "                 simple_renames= {}, # Replace entries that match the given key with the given value. {'A':'a'}\n",
    "                 split_renames= {} # Split entries that should be separate rows into two. {'a_b':['a','b']}\n",
    "                ):\n",
    "    \"Simplify renaming entries in a column for standardizaiton. Particularly useful for irrigation/managment entries which may be intended for humans not computers.\"\n",
    "    import pandas as pd\n",
    "    # simple renames\n",
    "    for e in simple_renames.keys():\n",
    "        mask = (df[col] == e)\n",
    "        df.loc[mask, col] = simple_renames[e]\n",
    "\n",
    "    # splits\n",
    "    # pull out the relevant multiname rows, copy, rename, append\n",
    "    for e in split_renames.keys():\n",
    "        mask = (df[col] == e)\n",
    "        temp = df.loc[mask, :] \n",
    "\n",
    "        df = df.loc[~mask, :]\n",
    "        for e2 in split_renames[e]:\n",
    "            temp2 = temp.copy()\n",
    "            temp2[col] = e2\n",
    "            df = df.merge(temp2, how = 'outer')\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  letters\n",
       "0       a\n",
       "1       b\n",
       "2       c\n",
       "3       d"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame.from_dict({'letters':['a', 'B', 'cd']})\n",
    "\n",
    "sanitize_col(df = df1, col = 'letters',\n",
    "                 simple_renames= {'B':'b'},\n",
    "                 split_renames= {'cd':['c', 'd']}\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility funciton to update notebook names?\n",
    "# rename notebooks\n",
    "# notebook name, cache path, cache dir name\n",
    "# also needs to search through and update cache path names for all notebooks that depend on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
